#
# Copyright The NOMAD Authors.
#
# This file is part of NOMAD. See https://nomad-lab.eu for further info.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import plotly.express as px
import plotly.graph_objects as go

import numpy as np
from PIL import Image
import base64
import io
import pint
import struct # for binary files

import os    

import re
import json

import zipfile

from nomad.datamodel.metainfo.plot import PlotSection
from nomad.datamodel.metainfo.eln import ELNMeasurement
#from nomad.parsing.tabular import TableData
from nomad.datamodel.data import UserReference, AuthorReference
from nomad.datamodel.metainfo.eln import ELNSubstance
from nomad.datamodel.metainfo.basesections.v1 import ReadableIdentifiers
from nomad.datamodel.metainfo.basesections.v1 import PureSubstance
from nomad.datamodel.metainfo.basesections.v1 import PureSubstanceSection
from nomad.datamodel.metainfo.eln import ELNInstrument
from nomad.datamodel.metainfo.eln import Chemical
from nomad.datamodel.data import EntryData


from typing import (
    TYPE_CHECKING,
)
from nomad.metainfo import (
    MSection,
    Package,
    SchemaPackage,
    Quantity,
    SubSection,
    MEnum,
    Reference,
    Datetime,
    Section,
)
from nomad.datamodel.data import (
    EntryData,
    ArchiveSection,
)
from nomad.datamodel.data import (
    EntryDataCategory,
)
from nomad.metainfo.metainfo import (
    Category,
)
from nomad.units import ureg
from nomad.datamodel.metainfo.plot import (
    PlotlyFigure,
    PlotSection,
)

# from nomad.metainfo.elasticsearch_extension import (
#     Elasticsearch,
#     material_entry_type,
#     entry_type as es_entry_type,
#     create_searchable_quantity,
# )

if TYPE_CHECKING:
    from nomad.datamodel.datamodel import (
        EntryArchive,
    )
    from structlog.stdlib import (
        BoundLogger,
    )
    

m_package = SchemaPackage(name='CRC1415 Sample ELN')


class DataFileError(Exception):
    """Custom exception for data file errors."""
    pass


class CRC1415Category(EntryDataCategory):
    """
    A category for all plugins defined in the `crc1415-plugin` plugin.
    """

    m_def = Category(label='CRC1415', categories=[EntryDataCategory])

class CRC1415Chemical(Chemical, EntryData, ArchiveSection):
    '''
    This is an example description for Chemical.
    A description can contain **markdown** markup and TeX formulas, like $\\sum\\limits_{i=0}^{n}$.
    '''
    m_def = Section(
        categories=[CRC1415Category],
        label='CRC1415-Chemical',
    )
    form = Quantity(
        type=MEnum(['crystalline solid', 'powder', 'solution']),
        a_eln={
            "component": "EnumEditQuantity"
        },
    )
    cas_number = Quantity(
        type=str,
        a_eln={
            "component": "StringEditQuantity"
        },
    )
    ec_number = Quantity(
        type=str,
        a_eln={
            "component": "StringEditQuantity"
        },
    )

    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -> None:
        '''
        The normalizer for the `CRC1415Chemical` class.

        Args:
            archive (EntryArchive): The archive containing the section that is being
            normalized.
            logger (BoundLogger): A structlog logger.
        '''
        super().normalize(archive, logger)


class Instrument(ELNInstrument, EntryData, ArchiveSection):
    '''
    Class autogenerated from yaml schema.
    '''
    m_def = Section()

    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -> None:
        '''
        The normalizer for the `Instrument` class.

        Args:
            archive (EntryArchive): The archive containing the section that is being
            normalized.
            logger (BoundLogger): A structlog logger.
        '''
        super().normalize(archive, logger)


class IRInstrument(Instrument, ArchiveSection):
    '''
    Class autogenerated from yaml schema.
    '''
    m_def = Section()


class Contributors(ArchiveSection):
    '''
    Class autogenerated from yaml schema.
    '''
    m_def = Section()
    Contributors = Quantity(
        type=AuthorReference,
        a_eln={
            "component": "AuthorEditQuantity"
        },
    )


#class XRDMeasurement(ELNMeasurement, TableData, PlotSection, ArchiveSection):
class MeasurementXRD(ELNMeasurement, PlotSection, ArchiveSection):
    '''
    Class for handling measurement of XRD.
    '''
    m_def = Section(
        categories=[CRC1415Category],
        label='CRC1415-Measurement-XRD',
        a_eln={
            "overview": True,
            "hide": [
                "name",
                "lab_id",
                "method",
                "samples",
                "measurement_identifiers"
            ],
            "properties": {
                "order": [
                    "tags",
                    "datetime",
                    "datetime_end",
                    "location",
                    "data_as_raw_or_xyd_file",
                    "data_as_xye_file",
                    "description"
                ]
            }
        },
        # a_plotly_graph_object=[
        #     {
        #         "data": [
        #             {
        #                 "x": "#Deg2Theta",
        #                 "y": "#Counts"
        #             }
        #         ],
        #         "layout": {
        #             "title": {
        #                 "text": "Counts over Degree 2Theta"
        #             }
        #         }
        #     }
        # ],
        )
    lab_id = Quantity(
        type=str,
        a_display={
            "visible": False
        },
    )
    
    datetime_end = Quantity(
        type=Datetime,
        description='The date and time when this activity has ended.',
        a_eln=dict(component='DateTimeEditQuantity', label='ending Time'),
    )
    
    data_as_raw_or_xyd_file = Quantity(
        type=str,
        description='''
        A reference to an uploaded .raw or .xyd produced by the XRD instrument.
        ''',
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity"
        },
    )
    
    data_as_xye_file = Quantity(
        type=str,
        description='''
        A reference to an uploaded .xye produced by XRD simulation.
        ''',
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity"
        },
    )
    
    Experiment_Wavelength = Quantity(
        type=np.float64,
        unit='nanometer',
        description='The wavelength of Cu K alpha (1.5406 Angstrom) used for XRD experiment.',
    )
    
    Simulation_Wavelength = Quantity(
        type=np.float64,
        unit='nanometer',
        description='The wavelength of Cu K alpha used for XRD simulations.',
    )
    
    Deg2Theta = Quantity(
        type=np.float64,
        a_tabular={
            "name": "Deg2Theta"
        },
        shape=["*"],
        unit='deg',
        description='The 2-theta range of the diffractogram',
    )
    Intensity = Quantity(
        type=np.float64,
        a_tabular={
            "name": "Counts"
        },
        shape=["*"],
        unit='dimensionless',
        description='The count at each 2-theta value, dimensionless',
    )
    
    Simulated_Deg2Theta = Quantity(
        type=np.float64,
        shape=["*"],
        unit='deg',
        description='The 2-theta range of the simulated diffractogram',
    )
    Simulated_Intensity = Quantity(
        type=np.float64,
        shape=["*"],
        unit='dimensionless',
        description='The simulated count at each 2-theta value, dimensionless',
    )
    
    def generate_plots(self) -> list[PlotlyFigure]:
        """
        Generate the plotly figures for the `MeasurementXRD` section.

        Returns:
            list[PlotlyFigure]: The plotly figures.
        """
        figures = []

        x_label = '2Theta'
        xaxis_title = f'{x_label} (Â°)'
        
        y_label = 'Normalized Intensity'
        yaxis_title = f'{y_label} (a.u.)'
        
        # line_linear = px.line(x=x, y=y/np.max(y))
        # 
        # line_linear.update_layout(
        #     title=f'{y_label} over {x_label}',
        #     xaxis_title=xaxis_title,
        #     yaxis_title=yaxis_title,
        #     xaxis=dict(
        #         fixedrange=False,
        #     ),
        #     yaxis=dict(
        #         fixedrange=False,
        #     ),
        #     template='plotly_white',
        # )
        # 
        # figures.append(
        #     PlotlyFigure(
        #         label=f'{y_label}-{x_label} linear plot',
        #         index=0,
        #         figure=line_linear.to_plotly_json(),
        #     ),
        # )
        
        config = {'displayModeBar': True}
        
        # Create the figure (for the moment: a blank graph)
        fig = go.Figure()

        # Add the scatter trace
        if self.data_as_raw_or_xyd_file:
            xExp = self.Deg2Theta.to('degree').magnitude
            yExp = self.Intensity.to('dimensionless').magnitude
            
            fig.add_trace(go.Scatter( 
                x=xExp, # Variable in the x-axis
                y=yExp/np.max(yExp), # Variable in the y-axis
                mode='lines', # This explicitly states that we want our observations to be represented by lines or use 'lines+markers'
                name='Experiment',
                hovertemplate='(x: %{x}, y: %{y})<extra></extra>',  # Custom hovertemplate
                # Properties associated with points 
                # marker=dict(
                #     size=12, # Size
                #     color='#cb1dd1', # Color
                #     opacity=0.8, # Point transparency 
                #     line=dict(width=1, color='black') # Properties of the edges
                # ),
            ))
            
        if self.data_as_xye_file:
            xSim = self.Simulated_Deg2Theta.to('degree').magnitude
            ySim = self.Simulated_Intensity.to('dimensionless').magnitude
            
            fig.add_trace(go.Scatter( 
                x=xSim, # Variable in the x-axis
                y=ySim/np.max(ySim), # Variable in the y-axis
                mode='lines', # This explicitly states that we want our observations to be represented by lines or use 'lines+markers'
                name='Simulation',
                hovertemplate='(x: %{x}, y: %{y})<extra></extra>',  # Custom hovertemplate
                # Properties associated with points 
                # marker=dict(
                #     size=12, # Size
                #     color='#cb1dd1', # Color
                #     opacity=0.8, # Point transparency 
                #     line=dict(width=1, color='black') # Properties of the edges
                # ),
            ))
        

        # Customize the layout
        fig.update_layout(
            title=f'{y_label} over {x_label}',
            xaxis_title=xaxis_title,
            yaxis_title=yaxis_title,
            showlegend=True,
            xaxis=dict(
                fixedrange=False,
            ),
            yaxis=dict(
                fixedrange=False,
            ),
            template='plotly_white',
            hovermode="x unified", # provides a dashed line and finds the closest point
            
        )
        
        figure_json = fig.to_plotly_json()
        figure_json['config'] = {'staticPlot': False, 'displayModeBar': True, 'scrollZoom': True, 'responsive': True, 'displaylogo': True, 'dragmode': True}
        
        figures.append(PlotlyFigure(label=f'{y_label}-{x_label} linear plot', figure=figure_json))

        return figures
    
    def unpack_repeated_bytes(self, byte_data, data_type, count):
        """
        Unpack a series of bytes into a tuple of the same data type.

        :param byte_data: The bytes to unpack.
        :param data_type: The format character for the data type (e.g., 'b' for signed char).
        :param count: The number of items to unpack.
        :return: A tuple of unpacked values.
        """
        # Create the format string based on the data type and count
        format_string = f'{count}{data_type}'
        
        # Unpack the byte data using the constructed format string
        return struct.unpack(format_string, byte_data)  
    
    def get_non_empty_chunks_separated_by_null(self, data_slice):
        """
        Get all non-empty chunks of data separated by NULL bytes.

        :param data_slice: The slice of data to split.
        :return: A list of bytes objects, each representing a non-empty chunk of data.
        """
        # Split the data by NULL bytes and filter out empty chunks
        return [chunk for chunk in data_slice.split(b'\x00') if chunk]
    
    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger'):
        """
        The normalize function of the `MeasurementXRD` section.

        Args:
            archive (EntryArchive): The archive containing the section that is being
            normalized.
            logger (BoundLogger): A structlog logger.
        """
        # super().normalize(archive, logger)
        
        try:
            # Check if any experimental (raw/xyd) or simulation (xye) file is provided
            if self.data_as_raw_or_xyd_file:
                # Check if the file has the correct extension
                if not self.data_as_raw_or_xyd_file.endswith('.xyd') and not self.data_as_raw_or_xyd_file.endswith('.raw'):
                    raise DataFileError(f"The file '{self.data_as_raw_or_xyd_file}' must have a .raw or .xyd extension.")
                    
                if self.data_as_raw_or_xyd_file.endswith('.xyd'):
                    # Otherwise parse the file
                    with archive.m_context.raw_file(self.data_as_raw_or_xyd_file) as xydfile:
                        # Load the data from the file
                        dataxydfile = np.loadtxt(xydfile)
                        
                        # Separate the columns into two variables and copy to 
                        self.Deg2Theta = ureg.Quantity(dataxydfile[:, 0], 'degree') # dataxydfile[:, 0]  # First column
                        self.Intensity = ureg.Quantity(dataxydfile[:, 1], 'dimensionless') #dataxydfile[:, 1]  # Second column
                        
                        # Otherwise create plot
                        # self.figures = self.generate_plots()
                        
                if self.data_as_raw_or_xyd_file.endswith('.raw'):
                    # Otherwise parse the file
                    with archive.m_context.raw_file(self.data_as_raw_or_xyd_file,'rb') as rawfile:
                        # Load the data from the file
                        contentrawfile = rawfile.read()
                        
                        ###
                        # File Type Version
                        ###
                        count = len(contentrawfile[0x00:0x0D + 1])//1 # Number of bytes to unpack
                        #print(count)
                        unpacked_data = self.unpack_repeated_bytes(contentrawfile[0x00:0x0D + 1], 'b', count)
                            
                        # Convert unpacked data to a string
                        string_output_file_type = ''.join(chr(b) for b in unpacked_data)
                        
                        if string_output_file_type != 'RAW_1.06Powdat':
                            logger.warn(f'This reader may not work for raw file with header: "{string_output_file_type}"')
                        
                        ###
                        # Date of Experiment
                        ###

                        datasplice = contentrawfile[0x0010:0x001F + 1]
                        count = len(datasplice)//1 # Number of bytes to unpack
                        #print(count)
                        unpacked_data = self.unpack_repeated_bytes(datasplice, 'b', count)
                            
                        # Convert unpacked data to a string
                        string_output_day = ''.join(chr(b) for b in unpacked_data)
                        
                        # Convert the unpacked data as a datetime object
                        from dateutil import parser as dataparser
                        dt = dataparser.parse(string_output_day)
                        
                        import pytz
                        
                        local_tz = pytz.timezone('Europe/Berlin')
                        target_tz = pytz.timezone('UTC')
                        
                        dt = local_tz.localize(dt) # set to Berlin time
                        dt = target_tz.normalize(dt) #transfer to UTC
                        
                        self.datetime = dt
                        
                        ###
                        # File Name And Comments?
                        ###
                        datasplice = contentrawfile[0x0020:0x012F + 1]
                        
                        # Get all chunks separated by NULL bytes in the data slice
                        chunks = self.get_non_empty_chunks_separated_by_null(datasplice)
                        
                        # Only add description if nothing is there
                        if not self.description:
                            self.description = ''
                            
                            # Print the result chunks
                            for i, chunk in enumerate(chunks):
                                #print(f'Chunk {i}: {chunk}')
                                count = len(chunk)//1 # Number of bytes to unpack (1 for char)
                                unpacked_data = self.unpack_repeated_bytes(chunk, 'b', count)
                                string_output_description = ''.join(chr(b) for b in unpacked_data)
                                # Print the unpacked data as a string
                                # The comments in the file is not needed - uncomment if necessary
                                # self.description += '<p>'+string_output_description + '</p>\n'
                                
                                
                        ###
                        # Experimental setup
                        # Copper K Alpha x-ray wavelength of 1.5406 Angstrom used by the experiment.
                        ###
                        datasplice = contentrawfile[0x0142:0x0146]
                        #'i': Integer (4 bytes)
                        #'f': Float (4 bytes)
                        #'d': Double (8 bytes)
                        #'h': Short (2 bytes)
                        count = len(datasplice)//4 # Number of bytes to unpack (4 for float)
                        #print(count)
                        unpacked_data = self.unpack_repeated_bytes(datasplice, 'f', count)
                        #print(unpacked_data)
                        
                        self.Experiment_Wavelength = ureg.Quantity(float(unpacked_data[0]), 'angstrom')
                        
                        
                        ###
                        # Start and End Time
                        ###
                        datasplice = contentrawfile[4*0x10000+0x0600:4*0x10000+0x0620]
                        #print(datasplice)
                        # Get all chunks separated by NULL bytes in the data slice
                        chunks = self.get_non_empty_chunks_separated_by_null(datasplice)
                            
                        # Print the result chunks
                        # for i, chunk in enumerate(chunks):
                        #     print(f'Chunk {i}: {chunk}')
                        #     count = len(chunk)//1 # Number of bytes to unpack (1 for char)
                        #     unpacked_data = self.unpack_repeated_bytes(chunk, 'b', count)
                        #     string_output_time = ''.join(chr(b) for b in unpacked_data)
                        #     # Print the unpacked data as a string
                        #     print(string_output_time)
                        #
                        # print(len(chunks), chunks[1])
                        
                        
                        if len(chunks) > 1:
                            count = len(chunks[1])//1 # Number of bytes to unpack (1 for char)
                            unpacked_data = self.unpack_repeated_bytes(chunks[1], 'b', count)
                            string_output_time = ''.join(chr(b) for b in unpacked_data)
                            
                            from dateutil import parser as dataparser
                            dt = dataparser.parse(string_output_time)
                            
                            import pytz
                        
                            local_tz = pytz.timezone('Europe/Berlin')
                            target_tz = pytz.timezone('UTC')
                            
                            dt = local_tz.localize(dt) # set to Berlin time
                            dt = target_tz.normalize(dt) #transfer to UTC
                            
                            self.datetime_end = dt
                        
                        ###
                        # Number of Data Entries
                        ###
                        datasplice = contentrawfile[4*0x10000+0x0622:4*0x10000+0x0624]
                        # 'i': Integer (4 bytes)
                        # 'f': Float (4 bytes)
                        # 'd': Double (8 bytes)
                        # 'h': Short (2 bytes)
                        count = len(datasplice)//2 # Number of bytes to unpack (1 for char)
                        #print(count)
                        unpacked_data = self.unpack_repeated_bytes(datasplice, 'h', count)
                        countDataEntries=int(unpacked_data[0])
                        # print(countDataEntries)
                        
                        ###
                        # x-range
                        ###
                        datasplice = contentrawfile[4*0x10000+0x062C:4*0x10000+0x0638]
                        
                        #'i': Integer (4 bytes)
                        #'f': Float (4 bytes)
                        #'d': Double (8 bytes)
                        #'h': Short (2 bytes)
                        count = len(datasplice)//4 # Number of bytes to unpack (1 for char)
                        #print(count)
                        unpacked_data = self.unpack_repeated_bytes(datasplice, 'f', count)
                        #print(unpacked_data)
                        
                        x_start = unpacked_data[0]
                        x_end = unpacked_data[2]

                        x_range = np.linspace(x_start, x_end, countDataEntries, True)
                        #print(type(x_range))
                        #print(x_range)
                        #print(len(x_range))
                        
                        self.Deg2Theta = ureg.Quantity(x_range, 'degree') # dataxydfile[:, 0]  # First column
                        
                        ###
                        # Data
                        ###

                        datasplice = contentrawfile[0x40800:0x40800+4*countDataEntries]
                        #'i': Integer (4 bytes)
                        #'f': Float (4 bytes)
                        #'d': Double (8 bytes)
                        count = len(datasplice)//4 # Number of bytes to unpack (1 for char)
                        #print(count)
                        unpacked_data = self.unpack_repeated_bytes(datasplice, 'i', count)

                        #print(unpacked_data)
                        #type(unpacked_data)
                        y_data = np.array(unpacked_data, dtype=np.int64)
                        #print(y_data)
                        #print(len(y_data))
                        
                        self.Intensity = ureg.Quantity(y_data, 'dimensionless') #dataxydfile[:, 1]  # Second column
                        
                        # Sanity check
                        if len(x_range) != len(y_data):
                            raise DataFileError(f"The data in file '{self.data_as_raw_or_xyd_file}' could not parsed. '{countDataEntries}' expected, but {len(y_data)} found!")
                        
                        # Create plot
                        #self.figures = self.generate_plots()
            
            
             # Check if any experimental (raw/xyd) or simulation (xye) file is provided
            if self.data_as_xye_file:
                # Check if the simulation file has the correct extension
                if not self.data_as_xye_file.endswith('.xye'):
                    raise DataFileError(f"The file '{self.data_as_xye_file}' must have a .xye extension.")
                    
                if self.data_as_xye_file.endswith('.xye'):
                    # Otherwise parse the file
                    with archive.m_context.raw_file(self.data_as_xye_file) as xyefile:
                        # The first line is the Cu K alpha wavelength
                        first_line = xyefile.readline().strip()
                        #first_value = float(first_line)
                        self.Simulation_Wavelength = ureg.Quantity(float(first_line), 'angstrom')
                        
                        # Load the data from the file, skipping the first line
                        dataxyefile = np.loadtxt(xyefile, skiprows=1)
                        
                        # Split the data into three distinct arrays
                        Sim_TwoTheta = dataxyefile[:, 0]  # 2Theta
                        Sim_Intensity = dataxyefile[:, 1]  # Simulated Intensity
                        #array3 = dataxyefile[:, 2]  # Simulated Intensity with Offset?!
                        self.Simulated_Deg2Theta = ureg.Quantity(Sim_TwoTheta, 'degree')
                        self.Simulated_Intensity = ureg.Quantity(Sim_Intensity, 'dimensionless')
                        
             # Check if any experimental (raw/xyd) or simulation (xye) file is provided
            if self.data_as_raw_or_xyd_file or self.data_as_xye_file:
                # Create plot
                self.figures = self.generate_plots()
            
        except Exception as e:
            logger.error('Invalid file parsing error.', exc_info=e)
            #logger.error('Invalid file extension for parsing.', exc_info=e)
        # In case something is odd here -> just return
        # if not self.results:
        #    return
        
        super().normalize(archive, logger)
        

class MeasurementIR(ELNMeasurement, PlotSection, ArchiveSection):
    '''
    Class for handling measurement of IR.
    '''
    m_def = Section(
        categories=[CRC1415Category],
        label='CRC1415-Measurement-IR',
        a_eln={
            "overview": True,
            "hide": [
                "name",
                "lab_id",
                "method",
                "samples",
                "measurement_identifiers"
            ],
            "properties": {
                "order": [
                    "tags",
                    "datetime",
                    "location",
                    "data_as_dpt_file",
                    "IR_Substance_Type",
                    "IR_Solvent",
                    "description"
                ]
            }
        },
        )
    lab_id = Quantity(
        type=str,
        a_display={
            "visible": False
        },
    )
    data_as_dpt_file = Quantity(
        type=str,
        description="A reference to an uploaded .dpt produced by the IR instrument.",
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity"
        },
    )
    
    IR_Substance_Type = Quantity(
        type=MEnum(['in solution', 'powder', 'KBr pellet', 'other']),
        description='The preparation condition of the sample in the IR experiment.',
        a_eln={
            "component": "RadioEnumEditQuantity",
            "label": "IR substance type"
        },
    )
    
    IR_Solvent  = Quantity(
        type=str,
        description='The solvent used for solving the sample in the IR experiment.',
        a_eln=dict(component='EnumEditQuantity', label='IR Solvent', suggestions=['Acetone', 'Acetonitrile (MeCN)', 'DMF (Dimethylformamide)', 'Ethanol', 'Isopropyl alcohol', 'Water']),
    )
    
    Wavenumber = Quantity(
        type=np.float64,
        shape=["*"],
        unit='1/cm',
        description='The wavenumber range of the spectrogram',
        a_eln={
            "defaultDisplayUnit": "1/cm",
        },
    )
    Transmittance = Quantity(
        type=np.float64,
        shape=["*"],
        unit='dimensionless',
        description='The transmittance at each wavenumber value, dimensionless',
    )
    
    def generate_plots(self) -> list[PlotlyFigure]:
        """
        Generate the plotly figures for the `MeasurementIR` section.

        Returns:
            list[PlotlyFigure]: The plotly figures.
        """
        figures = []
        #if self.wavelength is None:
        #    return figures

        x_label = 'Wavenumber'
        xaxis_title = f'{x_label} [{self.Wavenumber.units:~}]'
        #x = self.Wavenumber.to('1/cm').magnitude
        x = self.Wavenumber.to(self.Wavenumber.units).magnitude
        
        y_label = 'Transmittance'
        yaxis_title = f'{y_label} (a.u.)'
        y = self.Transmittance.to('dimensionless').magnitude
        
        fig = go.Figure()
        
        # Add the first line with markers
        fig.add_trace(go.Scatter(
            x=x,
            y=y,
            mode='lines',  # 'lines+markers' to show both lines and markers
            name='IR',         # Name of the first line
            line=dict(color='blue'),  # Line color
            hovertemplate='(x: %{x}, y: %{y})<extra></extra>',  # Custom hovertemplate
            marker=dict(size=10, symbol='circle')      # Marker size
        ))
        
        fig.update_layout(
            title=f'{y_label} over {x_label}',
            xaxis_title=xaxis_title,
            yaxis_title=yaxis_title,
            xaxis=dict(
                fixedrange=False,
            ),
            yaxis=dict(
                fixedrange=False,
            ),
            template='plotly_white',
            showlegend=True,
            hovermode="x unified",
        )
        
        figure_json = fig.to_plotly_json()
        
        figure_json['config'] = {'staticPlot': False, 'displayModeBar': True, 'scrollZoom': True, 'responsive': True, 'displaylogo': True, 'dragmode': True}
        
        figures.append(PlotlyFigure(label=f'{y_label}-{x_label} linear plot', figure=figure_json))
        
        self.figures = figures
        
        return figures
    
    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger'):
        """
        The normalize function of the `MeasurementIR` section.

        Args:
            archive (EntryArchive): The archive containing the section that is being
            normalized.
            logger (BoundLogger): A structlog logger.
        """
        
        try:
            # Check if any file is provided
            if self.data_as_dpt_file:
                # Check if the file has the correct extension
                if not self.data_as_dpt_file.endswith('.dpt'):
                    raise DataFileError(f"The file '{self.data_as_dpt_file}' must have a .dpt extension.")
            
                # Otherwise parse the file
                with archive.m_context.raw_file(self.data_as_dpt_file) as xyfile:
                    # Load the data from the file
                    dataxyfile = np.loadtxt(xyfile)
                    
                    # Separate the columns into two variables and copy to 
                    self.Wavenumber = ureg.Quantity(dataxyfile[:, 0], '1/cm') # dataxydfile[:, 0]  # First column
                    self.Transmittance = ureg.Quantity(dataxyfile[:, 1], 'dimensionless') #dataxydfile[:, 1]  # Second column
                    
                    # Otherwise create plot
                    self.figures = self.generate_plots()
        
        except Exception as e:
            logger.error('Invalid file extension for parsing.', exc_info=e)
        # In case something is odd here -> just return
        # if not self.results:
        #    return
        
        super().normalize(archive, logger)


class MeasurementSEM(ELNMeasurement, PlotSection, ArchiveSection):
    '''
    Class for handling measurement of SEM.
    '''
    m_def = Section(
        categories=[CRC1415Category],
        label='CRC1415-Measurement-SEM',
        a_eln={
            "overview": True,
            "hide": [
                "name",
                "lab_id",
                "method",
                "samples",
                "measurement_identifiers"
            ],
            "properties": {
                "order": [
                    "tags",
                    "datetime",
                    "location",
                    "name",
                    "data_as_tif_or_tiff_file",
                    "auxiliary_data_file",
                    "SEM_Accelerating_Voltage",
                    "SEM_Magnification",
                    "description"
                ]
            }
        },
        )
    lab_id = Quantity(
        type=str,
        a_display={
            "visible": False
        },
    )
    data_as_tif_or_tiff_file = Quantity(
        type=str,
        shape=["*"],
        description='''
        A reference to an uploaded .tif produced by the SEM instrument.
        ''',
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity"
        },
        repeats=True,
    )
    
    auxiliary_data_file = Quantity(
        type=str,
        description='''
        A reference to an uploaded .tif produced by the SEM instrument.
        ''',
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity",
            "label": "Auxiliary metadata file as .txt"
        },
    )
    
    SEM_Accelerating_Voltage = Quantity(
        type=np.float64,
        unit='volt',
        description='The voltage applied in the SEM experiment, volt.',
        a_eln=dict(component='NumberEditQuantity', label='SEM: Accelerating Voltage', defaultDisplayUnit= 'volt'),
    )
    
    SEM_Magnification = Quantity(
        type=np.float64,
        unit='dimensionless',
        description='The magnification used in the SEM experiment, dimensionless.',
        a_eln=dict(component='NumberEditQuantity', label='SEM: Magnification', defaultDisplayUnit= 'dimensionless'),
    )
    
    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger'):
        """
        The normalize function of the `MeasurementSEM` section.

        Args:
            archive (EntryArchive): The archive containing the section that is being
            normalized.
            logger (BoundLogger): A structlog logger.
        """
        
        try:
            
            # Check if any file is provided
            if self.data_as_tif_or_tiff_file:
                self.figures = []
                # Loop over all filenames
                for data_file in self.data_as_tif_or_tiff_file: #.split(" "):
                    # Check if the file has the correct extension
                    if not data_file.endswith('.tif'):
                        if not data_file.endswith('.tiff'):
                            raise DataFileError(f"The file '{data_file}' must have a .tif or .tiff extension.")
                
                    # Otherwise parse the file as binary
                    # with archive.m_context.raw_file(data_file, 'rb') as imagefile:
                    #    archive.m_context.raw_file(data_file) as xyfile:
                    with archive.m_context.raw_file(data_file, 'rb') as imagefile:
                        with Image.open(imagefile) as img:
                            # Get the size of the image
                            img_width, img_height = img.size
                            # print(f"Width: {img_width}, Height: {img_height}")
                            
                             # Convert the image to RGB (necessary for JPEG)
                            img = img.convert('RGB')
                            # Create a BytesIO object to hold the image data
                            buffered = io.BytesIO()
                            # Save the image to the BytesIO object in JPEG format
                            img.save(buffered, format="JPEG")
                            # Get the byte data
                            img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')
                            # Create the URI image string
                            uri = f"data:image/jpeg;base64,{img_str}"
                            
                            # see https://plotly.com/python/images/#zoom-on-static-images
                            fig = go.Figure()
                            scale_factor = 0.5
                            fig.add_trace(
                                go.Scatter(
                                    x=[0, img_width * scale_factor],
                                    y=[0, img_height * scale_factor],
                                    mode="markers",
                                    marker_opacity=0
                                )
                            )
                            # Configure axes
                            fig.update_xaxes(
                                visible=False,
                                range=[0, img_width * scale_factor]
                            )

                            fig.update_yaxes(
                                visible=False,
                                range=[0, img_height * scale_factor],
                                # the scaleanchor attribute ensures that the aspect ratio stays constant
                                scaleanchor="x"
                            )
                            
                            # Add image
                            fig.add_layout_image(
                                dict(
                                    x=0,
                                    sizex=img_width * scale_factor,
                                    y=img_height * scale_factor,
                                    sizey=img_height * scale_factor,
                                    xref="x",
                                    yref="y",
                                    opacity=1.0,
                                    layer="below",
                                    sizing="stretch",
                                    source=uri)
                            )
                            # Configure other layout
                            fig.update_layout(
                                width=img_width * scale_factor,
                                height=img_height * scale_factor,
                                margin={"l": 0, "r": 0, "t": 0, "b": 0},
                            )
                            
                            figure_json = fig.to_plotly_json()
                            figure_json['config'] = {'staticPlot': True, 'displayModeBar': False, 'scrollZoom': True, 'responsive': False, 'displaylogo': False, 'dragmode': False}
                            #self.figures.append(PlotlyFigure(label='Measurement SEM', index=0, figure=figure_json))
                            # label=f'{y_label} linear plot',
                            self.figures.append(PlotlyFigure(label=f'Measurement SEM: {data_file}', figure=figure_json))
                            #self.figures = [PlotlyFigure(label=f'Measurement SEM: {data_file}', index=0, figure=figure_json)]
                
            if self.auxiliary_data_file:
                if not self.auxiliary_data_file.endswith('.txt'):
                    raise DataFileError(f"The file '{self.auxiliary_data_file}' must have a .txt extension.")
                
                # Otherwise parse the file as binary
                with archive.m_context.raw_file(self.auxiliary_data_file, 'r') as txtfile:
                    
                    text = txtfile.read()
                    
                    AV = re.search(r'(AcceleratingVoltage)=([\d.]+)\s+(\w+)', text, re.IGNORECASE)
                    
                    if AV:
                        self.SEM_Accelerating_Voltage = ureg.Quantity(float(AV.group(2)), AV.group(3).lower()) # decimal number, volt
                    
                    MG = re.search(r'(Magnification)=([\d.]+)', text, re.IGNORECASE)
                    
                    if MG:
                        self.SEM_Magnification = ureg.Quantity(float(MG.group(2)), 'dimensionless') # decimal number
                        
                    DATE = re.search(r'Date=([\d.]+)/([\d.]+)/([\d.]+)', text, re.IGNORECASE) # MM/DD/YYYY
                    TIME = re.search(r'Time=([\d.]+):([\d.]+):([\d.]+)', text, re.IGNORECASE) # HH:MM:SS
                    
                    if DATE and TIME:
                        import pytz
                        import datetime
                        # Should follow:    DD-MM-YYYY HH:MM:SS in Berlin/Europe time zone
                        exp_time = datetime.datetime(int(DATE.group(3)),
                                                     int(DATE.group(1)),
                                                     int(DATE.group(2)),
                                                     int(TIME.group(1)),
                                                     int(TIME.group(2)),
                                                     int(TIME.group(3)))
                        
                        local_tz = pytz.timezone('Europe/Berlin')
                        target_tz = pytz.timezone('UTC')
                        
                        exp_time = local_tz.localize(exp_time) # set to Berlin time
                        exp_time = target_tz.normalize(exp_time) #transfer to UTC
                        
                        self.datetime = exp_time
                
                
                
                
            
        except Exception as e:
            logger.error('Invalid file extension for parsing.', exc_info=e)
        # In case something is odd here -> just return
        # if not self.results:
        #    return
        
        # Otherwise create plot
        #self.figures = self.generate_plots()
        super().normalize(archive, logger)


class MeasurementTEM(ELNMeasurement, PlotSection, ArchiveSection):
    '''
    Class for handling measurement of SEM.
    '''
    m_def = Section(
        categories=[CRC1415Category],
        label='CRC1415-Measurement-TEM',
        a_eln={
            "overview": True,
            "hide": [
                "name",
                "lab_id",
                "method",
                "samples",
                "measurement_identifiers"
            ],
            "properties": {
                "order": [
                    "tags",
                    "datetime",
                    "location",
                    "name",
                    "data_as_tif_or_tiff_file",
                    "auxiliary_data_file",
                    #"TEM_Accelerating_Voltage",
                    #"TEM_Magnification",
                    "description"
                ]
            }
        },
        )
    lab_id = Quantity(
        type=str,
        a_display={
            "visible": False
        },
    )
    data_as_tif_or_tiff_file = Quantity(
        type=str,
        shape=["*"],
        description='''
        A reference to an uploaded .tif produced by the TEM instrument.
        ''',
        a_tabular_parser={
            "parsing_options": {
                "sep": "\\t",
                "comment": "#"
            }
        },
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity"
        },
        repeats=True,
    )
    
    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger'):
        """
        The normalize function of the `MeasurementTEM` section.

        Args:
            archive (EntryArchive): The archive containing the section that is being
            normalized.
            logger (BoundLogger): A structlog logger.
        """
        
        try:
            
            # Check if any file is provided
            if self.data_as_tif_or_tiff_file:
                self.figures = []
                # Loop over all filenames
                for data_file in self.data_as_tif_or_tiff_file: #.split(" "):
                    # Check if the file has the correct extension
                    if not data_file.endswith('.tif'):
                        if not data_file.endswith('.tiff'):
                            raise DataFileError(f"The file '{data_file}' must have a .tif or .tiff extension.")
                
                    # Otherwise parse the file as binary
                    # with archive.m_context.raw_file(data_file, 'rb') as imagefile:
                    #    archive.m_context.raw_file(data_file) as xyfile:
                    with archive.m_context.raw_file(data_file, 'rb') as imagefile:
                        with Image.open(imagefile) as img:
                            # Get the size of the image
                            img_width, img_height = img.size
                            # print(f"Width: {img_width}, Height: {img_height}")
                            
                             # Convert the image to RGB (necessary for JPEG)
                            img = img.convert('RGB')
                            # Create a BytesIO object to hold the image data
                            buffered = io.BytesIO()
                            # Save the image to the BytesIO object in JPEG format
                            img.save(buffered, format="JPEG")
                            # Get the byte data
                            img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')
                            # Create the URI image string
                            uri = f"data:image/jpeg;base64,{img_str}"
                            
                            # see https://plotly.com/python/images/#zoom-on-static-images
                            fig = go.Figure()
                            # As TEM images are usually very big we scale it
                            # down to fixed size
                            scale_factor = 800.0/img_width 
                            fig.add_trace(
                                go.Scatter(
                                    x=[0, img_width * scale_factor],
                                    y=[0, img_height * scale_factor],
                                    mode="markers",
                                    marker_opacity=0
                                )
                            )
                            # Configure axes
                            fig.update_xaxes(
                                visible=False,
                                range=[0, img_width * scale_factor]
                            )

                            fig.update_yaxes(
                                visible=False,
                                range=[0, img_height * scale_factor],
                                # the scaleanchor attribute ensures that the aspect ratio stays constant
                                scaleanchor="x"
                            )
                            
                            # Add image
                            fig.add_layout_image(
                                dict(
                                    x=0,
                                    sizex=img_width * scale_factor,
                                    y=img_height * scale_factor,
                                    sizey=img_height * scale_factor,
                                    xref="x",
                                    yref="y",
                                    opacity=1.0,
                                    layer="below",
                                    sizing="stretch",
                                    source=uri)
                            )
                            # Configure other layout
                            fig.update_layout(
                                width=img_width * scale_factor,
                                height=img_height * scale_factor,
                                margin={"l": 0, "r": 0, "t": 0, "b": 0},
                            )
                            
                            figure_json = fig.to_plotly_json()
                            figure_json['config'] = {'staticPlot': True, 'displayModeBar': False, 'scrollZoom': True, 'responsive': False, 'displaylogo': False, 'dragmode': False}
                            #self.figures.append(PlotlyFigure(label='Measurement SEM', index=0, figure=figure_json))
                            # label=f'{y_label} linear plot',
                            self.figures.append(PlotlyFigure(label=f'Measurement TEM: {data_file}', figure=figure_json))
                            #self.figures = [PlotlyFigure(label=f'Measurement SEM: {data_file}', index=0, figure=figure_json)]
            
        except Exception as e:
            logger.error('Invalid file extension for parsing.', exc_info=e)
        # In case something is odd here -> just return
        # if not self.results:
        #    return
        
        # Otherwise create plot
        #self.figures = self.generate_plots()
        super().normalize(archive, logger)

class RamanData(ArchiveSection):
    """General data section for Raman spectroscopy"""

    m_def = Section(
        label_quantity='name',
        a_eln={
            # "overview": False,
            # "hide": [
            #     "name",
            #     "lab_id",
            #     "method",
            #     "samples",
            #     "measurement_identifiers"
            # ],
            "properties": {
                "order": [
                    "name",
                    "data_as_tvf_or_txt_file",
                ]
            }
        },
    )
    
    name = Quantity(
        type=str,
        #default='TestName',
        description='Name of the section or Raman measurement',
        a_eln={'component': 'StringEditQuantity'},
    )
    
    
    data_as_tvf_or_txt_file = Quantity(
        type=str,
        description="A reference to an uploaded TriVista .tvf or .txt file produced by the Raman instrument.",
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity"
        },
    )
        
    Raman_shift = Quantity(
        type=np.float64,
        shape=["*"],
        unit='1/centimeter',
        description='The wavenumber range of the spectrogram.',
    )
    Intensity = Quantity(
        type=np.float64,
        shape=["*"],
        unit='dimensionless',
        description='The intensity or counts at each Raman wavenumber value, dimensionless',
    )
    
class ReferencedRamanData(ArchiveSection):
    """General data section for Raman spectroscopy"""

    m_def = Section(
        label_quantity='name',
        a_eln={
            # "overview": False,
            # "hide": [
            #     "name",
            #     "lab_id",
            #     "method",
            #     "samples",
            #     "measurement_identifiers"
            # ],
            "properties": {
                "order": [
                    "name",
                    "Reference_to_Raman_Data",
                ]
            }
        },
    )
    
    name = Quantity(
        type=str,
        #default='TestName',
        description='Name of the section or brief title',
        a_eln={'component': 'StringEditQuantity'},
    )
    
    Reference_to_Raman_Data = Quantity(
        type='RamanData',
        description='If you want to plot the data, then reference it here.',
        a_eln={
            "component": "ReferenceEditQuantity"
        },
        shape=["*"],
    )
    
    

class MeasurementRaman(ELNMeasurement, PlotSection, ArchiveSection):
    '''
    Class for handling measurement of Raman spectroscopy.
    '''
    m_def = Section(
        categories=[CRC1415Category],
        label='CRC1415-Measurement-Raman',
        a_eln={
            "overview": True,
            "hide": [
                #"name",
                "lab_id",
                "method",
                "samples",
                "measurement_identifiers"
            ],
            "properties": {
                "order": [
                    "tags",
                    "datetime",
                    "name",
                    "location",
                    "data_as_tvb_file",
                    "processed_data_as_zip_file",
                    "Laser_Excitation_Wavelength",
                    "Laser_Power",
                    "Ramification_Objective",
                    "Groove_Density",
                    "Accumulation_Time",
                    "No_of_Accumulations",
                    "description",
                    "Raman_data_entries",
                    "Raman_processed_data_entries",
                    "Raman_referenced_data_entries",
                ]
            }
        },
        )
            
    lab_id = Quantity(
        type=str,
        a_display={
            "visible": False
        },
    )
        
    name = Quantity(
        type=str,
        #default='TestName',
        description='Name of the section of Raman measurement',
        a_eln={'component': 'StringEditQuantity', 'label': 'Raman: Brief title of the measurement'},
    )
    
    Laser_Excitation_Wavelength = Quantity(
        type=np.float64,
        unit='nanometer',
        description='The wavelength of the laser for Raman spectroscopy, nanometer.',
        a_eln=dict(component='NumberEditQuantity', label='Raman: Laser Excitation Wavelength', defaultDisplayUnit= 'nanometer'),
    )
    
    Laser_Power = Quantity(
        type=np.float64,
        unit='milliwatt',
        description='The power of the laser for Raman spectroscopy, mW.',
        a_eln=dict(component='NumberEditQuantity', label='Raman: Laser Power', defaultDisplayUnit= 'milliwatt'),
    )
    
    Ramification_Objective = Quantity(
        type=np.float64,
        unit='dimensionless',
        description='The ramification of the objective for the laser in Raman spectroscopy, dimensionless.',
        a_eln=dict(component='NumberEditQuantity', label='Raman: Ramification Objective', defaultDisplayUnit= 'dimensionless'),
    )
    
    Groove_Density = Quantity(
        type=np.float64,
        unit='1/millimeter',
        description='The number of grooves per area of a grating in Raman spectroscopy, grooves/millimeter.',
        a_eln=dict(component='NumberEditQuantity', label='Raman: Groove Density', defaultDisplayUnit= '1/millimeter'),
    )
    
    Accumulation_Time = Quantity(
        type=np.float64,
        unit='second',
        description='The time intervall to average the measurement in the accumation step in Raman spectroscopy, second.',
        a_eln=dict(component='NumberEditQuantity', label='Raman: Exposure Time per Accumulation', defaultDisplayUnit= 'second'),
    )
    
    No_of_Accumulations = Quantity(
        type=np.int32,
        unit='dimensionless',
        description='The number of accumulations for one frame during the measurement in Raman spectroscopy, dimensionless.',
        a_eln=dict(component='NumberEditQuantity', label='Raman: Number of Accumulations per Frame', defaultDisplayUnit= 'dimensionless'),
    )
    
    data_as_tvb_file = Quantity(
        type=str,
        description="A reference to an uploaded TriVista binary .tvb produced by the Raman instrument.",
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity",
            "label": "Raman data as .tvb file"
        },
    )
        
    processed_data_as_zip_file = Quantity(
        type=str,
        description="A reference to an uploaded .zip archive of processed data containing plain x-y-value table as .txt files.",
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity",
            "label": "Processed Raman data as .zip archive"
        },
    )
    
    
    Raman_data_entries = SubSection(section_def=RamanData, repeats=True)
    
    Raman_processed_data_entries = SubSection(section_def=RamanData, repeats=True)
    
    Raman_referenced_data_entries = SubSection(section_def=ReferencedRamanData)#, repeats=True)
    
    
    def generate_plots(self) -> list[PlotlyFigure]:
        """
        Generate the plotly figures for the `MeasurementRaman` section.

        Returns:
            list[PlotlyFigure]: The plotly figures.
        """
        figures = []
        ##
        # Create the figure - messured data
        ##
        if self.Raman_data_entries:
            fig = go.Figure()
            
            #for r_d_entries in self.Raman_data_entries:
            for idx, r_d_entries in enumerate(self.Raman_data_entries):
                #print(f"Index {idx}/{(len(self.Raman_data_entries) - 1)}: {r_d_entries}")
                # Add line plots
                x = r_d_entries.Raman_shift.to('1/centimeter').magnitude
                y = r_d_entries.Intensity.to('dimensionless').magnitude
                
                
                # Get the Viridis color scale
                viridis_colors = px.colors.sequential.Viridis
                
                color_index_line = int(idx / (len(self.Raman_data_entries)-1) * (len(viridis_colors) - 1)) if len(self.Raman_data_entries) > 1 else 0
                
                fig.add_trace(go.Scatter(
                    x=x,
                    y=y,
                    mode='lines',
                    name=f'frame: {idx}',
                    line=dict(color=viridis_colors[color_index_line]), # int(idx / (len(self.Raman_data_entries)) * (len(viridis_colors) - 1))]),
                    hovertemplate='(x: %{x}, y: %{y})<extra></extra>',
                ))

            # exemply use the first entry for the units
            x_label = 'Raman shift'
            xaxis_title = f'{x_label} ({self.Raman_data_entries[0].Raman_shift.units:~})'#(1/cm)' the ':~' gives the short form
            
            y_label = 'Intensity'
            yaxis_title = f'{y_label} (a.u.)'
            
            fig.update_layout(
                title=f'{y_label} over {x_label}',
                xaxis_title=xaxis_title,
                yaxis_title=yaxis_title,
                xaxis=dict(
                    fixedrange=False,
                ),
                yaxis=dict(
                    fixedrange=False,
                ),
                #legend=dict(yanchor='top', y=0.99, xanchor='left', x=0.01),
                template='plotly_white',
                showlegend=True,
                hovermode="x unified",
            )

            # figures.append(
            #     PlotlyFigure(
            #         label=f'{y_label}-{x_label} linear plot',
            #         #index=0,
            #         figure=fig.to_plotly_json(),
            #     ),
            # )
            
            figure_json = fig.to_plotly_json()
            figure_json['config'] = {'staticPlot': False, 'displayModeBar': True, 'scrollZoom': True, 'responsive': True, 'displaylogo': True, 'dragmode': True}
            
            figures.append(
                PlotlyFigure(
                    label=f'{y_label}-{x_label} linear plot',
                    figure=figure_json
                )
            )
        
        ##
        # Create the figure - processed data
        ##
        if self.Raman_processed_data_entries:
            figProcessedData = go.Figure()
            
            #for r_d_entries in self.Raman_data_entries:
            for idx, r_d_entries in enumerate(self.Raman_processed_data_entries):
                #print(f"Index {idx}/{(len(self.Raman_data_entries) - 1)}: {r_d_entries}")
                # Add line plots
                x = r_d_entries.Raman_shift.to(r_d_entries.Raman_shift.units).magnitude
                y = r_d_entries.Intensity.to('dimensionless').magnitude
                
                
                # Get the Viridis color scale
                viridis_colors = px.colors.sequential.Viridis
                
                color_index_line = int(idx / (len(self.Raman_processed_data_entries)-1) * (len(viridis_colors) - 1)) if len(self.Raman_processed_data_entries) > 1 else 0
                
                figProcessedData.add_trace(go.Scatter(
                    x=x,
                    y=y,
                    mode='lines',
                    name=f'frame: {idx}',
                    line=dict(color=viridis_colors[color_index_line]), # int(idx / (len(self.Raman_data_entries)) * (len(viridis_colors) - 1))]),
                    hovertemplate='(x: %{x}, y: %{y})<extra></extra>',
                ))

            # exemply use the first entry for the units
            x_label = 'Raman shift'
            xaxis_title = f'{x_label} ({self.Raman_processed_data_entries[0].Raman_shift.units:~})'#(1/cm)' the ':~' gives the short form
            
            y_label = 'Intensity'
            yaxis_title = f'{y_label} (a.u.)'
            
            figProcessedData.update_layout(
                title=f'Processed: {y_label} over {x_label}',
                xaxis_title=xaxis_title,
                yaxis_title=yaxis_title,
                xaxis=dict(
                    fixedrange=False,
                ),
                yaxis=dict(
                    fixedrange=False,
                ),
                #legend=dict(yanchor='top', y=0.99, xanchor='left', x=0.01),
                template='plotly_white',
                showlegend=True,
                hovermode="x unified",
            )

            # figures.append(
            #     PlotlyFigure(
            #         label=f'{y_label}-{x_label} linear plot',
            #         #index=0,
            #         figure=fig.to_plotly_json(),
            #     ),
            # )
            
            figure_json = figProcessedData.to_plotly_json()
            figure_json['config'] = {'staticPlot': False, 'displayModeBar': True, 'scrollZoom': True, 'responsive': True, 'displaylogo': True, 'dragmode': True}
            
            figures.append(
                PlotlyFigure(
                    label=f'Processed: {y_label}-{x_label} linear plot',
                    figure=figure_json
                )
            )
        
        ##
        # Create the figure - referenced data
        ##
        if self.Raman_referenced_data_entries:
            
            # if theres is any referenced data
            if self.Raman_referenced_data_entries.Reference_to_Raman_Data:
                
                figReferencedData = go.Figure()
                
                for idx, r_d_entries in enumerate(self.Raman_referenced_data_entries.Reference_to_Raman_Data):
                    #print(f"Index {idx}/{(len(self.Raman_data_entries) - 1)}: {r_d_entries}")
                    # Add line plots
                    x = r_d_entries.Raman_shift.to(r_d_entries.Raman_shift.units).magnitude
                    y = r_d_entries.Intensity.to('dimensionless').magnitude
                    
                    
                    # Get the Viridis color scale
                    viridis_colors = px.colors.sequential.Viridis
                    
                    color_index_line = int(idx / (len(self.Raman_referenced_data_entries.Reference_to_Raman_Data)-1) * (len(viridis_colors) - 1)) if len(self.Raman_referenced_data_entries.Reference_to_Raman_Data) > 1 else 0
                    
                    figReferencedData.add_trace(go.Scatter(
                        x=x,
                        y=y,
                        mode='lines',
                        name=f'frame: {r_d_entries.name}',
                        line=dict(color=viridis_colors[color_index_line]), # int(idx / (len(self.Raman_data_entries)) * (len(viridis_colors) - 1))]),
                        hovertemplate='(x: %{x}, y: %{y})<extra></extra>',
                    ))

                # exemply use the first entry for the units
                x_label = 'Raman shift'
                xaxis_title = f'{x_label} (1/cm)'#(1/cm)' the ':~' gives the short form
                
                y_label = 'Intensity'
                yaxis_title = f'{y_label} (a.u.)'
                
                figReferencedData.update_layout(
                    title=f'Compare: {y_label} over {x_label}',
                    xaxis_title=xaxis_title,
                    yaxis_title=yaxis_title,
                    xaxis=dict(
                        fixedrange=False,
                    ),
                    yaxis=dict(
                        fixedrange=False,
                    ),
                    #legend=dict(yanchor='top', y=0.99, xanchor='left', x=0.01),
                    template='plotly_white',
                    showlegend=True,
                    hovermode="x unified",
                )

                # figures.append(
                #     PlotlyFigure(
                #         label=f'{y_label}-{x_label} linear plot',
                #         #index=0,
                #         figure=fig.to_plotly_json(),
                #     ),
                # )
                
                figure_json = figReferencedData.to_plotly_json()
                figure_json['config'] = {'staticPlot': False, 'displayModeBar': True, 'scrollZoom': True, 'responsive': True, 'displaylogo': True, 'dragmode': True}
                
                figures.append(
                    PlotlyFigure(
                        label=f'Compare: {y_label}-{x_label} linear plot',
                        figure=figure_json
                    )
                )
        
        
        self.figures = figures

        return figures
    
    def unpack_repeated_bytes(self, byte_data, data_type, count, littleEndianEncoding=True):
        """
        Unpack a series of bytes into a tuple of the same data type.
        'i': Integer (4 bytes)
        'I': Unsigned Int (4 bytes)
        'l': Long (4 bytes)
        'L': Long (8 bytes)
        'f': Float (4 bytes)
        'd': Double (8 bytes)
        'h': Short (2 bytes)
        'b': Signed char (1 byte)
        'B': Unsigned char (1 byte)
        'q': Long long (8 bytes)
        'Q': Unsigned long long (8 bytes)

        :param byte_data: The bytes to unpack.
        :param data_type: The format character for the data type (e.g., 'b' for signed char).
        :param count: The number of items to unpack.
        :param littleEndianEncoding: Flag to determine if the data is in little-endian format.
        :return: A tuple of unpacked values.
        """
        # Determine the endianness based on the flag
        endianness = '<' if littleEndianEncoding else '>'
        
        # Create the format string based on the data type, count, and endianness
        format_string = f'{endianness}{count}{data_type}'
        
        # Unpack the byte data using the constructed format string
        return struct.unpack(format_string, byte_data)
    
    def get_non_empty_chunks_separated_by_null(self, data_slice):
        """
        Get all non-empty chunks of data separated by NULL bytes.

        :param data_slice: The slice of data to split.
        :return: A list of bytes objects, each representing a non-empty chunk of data.
        """
        # Split the data by NULL bytes and filter out empty chunks
        return [chunk for chunk in data_slice.split(b'\x00') if chunk]
    
    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger'):
        """
        The normalize function of the `MeasurementRaman` section.

        Args:
            archive (EntryArchive): The archive containing the section that is being
            normalized.
            logger (BoundLogger): A structlog logger.
        """
        # super().normalize(archive, logger)
        try:
            #Check if there's any TriVista binary .tvb file provided in main section
            if self.data_as_tvb_file:
                if not self.data_as_tvb_file.endswith('.tvb'):
                    raise DataFileError(f"The file '{self.data_as_tvb_file}' must have a .tvb extension.")
                
                # Otherwise parse the file
                with archive.m_context.raw_file(self.data_as_tvb_file,'rb') as tvbfile:
                    # Load the data from the file
                    contentTVBfile = tvbfile.read()
                    
                    ###
                    # File Type Version
                    ###
                    datasplice = contentTVBfile[0x0000:0x003] # this should be 'tvb'
                    # 'b': Signed char (1 byte)
                    count = len(datasplice)//1 # Number of bytes to unpack (1 for char)
                    #print(count)
                    unpacked_data = self.unpack_repeated_bytes(datasplice, 'b', count)
                    string_output_file_type = ''.join(chr(b) for b in unpacked_data)
                    #print(string_output_file_type)
                    
                    if string_output_file_type != 'tvb':
                        logger.error(f'This reader may not work for tvb file with header: "{string_output_file_type}"')
                    
                    ###
                    # File Info - Frames and Dataset Length
                    ###
                    datasplice = contentTVBfile[0x0004:0x0016] 
                    # 'h': short (2 byte)
                    count = len(datasplice)//2 # Number of bytes to unpack (1 for char)
                    #print(count)
                    unpacked_data = self.unpack_repeated_bytes(datasplice, 'h', count)
                    #print(unpacked_data)
                    
                    numDatasetLength = int(unpacked_data[1])
                    numFrames = int(unpacked_data[5])
                    #print(numDatasetLength, numFrames)
                    
                    ###
                    # LaserExcitationWavelength
                    ###
                    datasplice = contentTVBfile[0x0025:0x002D]
                    # 'd': double (8 byte)
                    count = len(datasplice)//8 # Number of bytes to unpack (1 for char)
                    #print(count)
                    unpacked_data = self.unpack_repeated_bytes(datasplice, 'd', count)
                    #print(unpacked_data)
                    
                    LaserExcitationWavelength = float(unpacked_data[0])
                    
                    self.Laser_Excitation_Wavelength = ureg.Quantity(LaserExcitationWavelength, 'nanometer')
                        
                    #print(LaserExcitationWavelength)
                    
                    ###
                    # Number of Raman Wavelength entries = NRWE
                    ###
                    datasplice = contentTVBfile[0x002D:0x0031] 
                    # 'I': unsigned integer (4 byte)
                    count = len(datasplice)//4 # Number of bytes to unpack (1 for char)
                    #print(count)
                    unpacked_data = self.unpack_repeated_bytes(datasplice, 'I', count)
                    #print(unpacked_data)
                    
                    NRWE = int(unpacked_data[0])
                    # print(NRWE)
                    
                    ###
                    # List of Raman Wavelength [in nm] convert to Raman Shift = Raman Wavenumber [1/nm]
                    ###
                    datasplice = contentTVBfile[0x0031:0x0031+4*NRWE] 
                    # 'f': float (4 byte)
                    count = len(datasplice)//4 # Number of bytes to unpack (1 for char)
                    #print(count)
                    unpacked_data = self.unpack_repeated_bytes(datasplice, 'f', count)
                    #print(unpacked_data)
                    
                    import numpy as np
                    RamanWavenumber = (1.0/LaserExcitationWavelength-1.0/np.asarray(unpacked_data, dtype=np.float64)) # in 1/nm
                    #print(RamanWavenumber)
                    
                    ###
                    # Character Length of XML section = CLXML
                    ###
                    datasplice = contentTVBfile[0x1534:0x1538] 
                    # 'I': unsigned integer (4 byte)
                    count = len(datasplice)//4 # Number of bytes to unpack (1 for char)
                    #print(count)
                    unpacked_data = self.unpack_repeated_bytes(datasplice, 'I', count)
                    #print(unpacked_data)
                    
                    CLXML = int(unpacked_data[0])
                    #print(CLXML)
                    
                    ###
                    # XML part -> Date, LaserPower, Ramification Objective, Groove Density, Exposure_time, Number_Accumlations
                    ###
                    datasplice = contentTVBfile[0x1538:0x1538+1*CLXML]
                    # 'b': Signed char (1 byte)
                    count = len(datasplice)//1 # Number of bytes to unpack (1 for char)
                    #print(count)
                    unpacked_data = self.unpack_repeated_bytes(datasplice, 'b', count)
                    string_output_xml = ''.join(chr(b) for b in unpacked_data)
                    #print(string_output_xml)
                    
                    import xmltodict, json
                    dataxmlfile = xmltodict.parse(string_output_xml)
                    string_experiment_time = dataxmlfile['Info']['Groups']['Group'][0]['Items']['Item']['Value']
                    
                    
                    if dataxmlfile['Info']['Groups']['Group'][1]['Items']['Item'][2]['Name'] == 'Laser-Power':
                        laser_power_str = dataxmlfile['Info']['Groups']['Group'][1]['Items']['Item'][2]['Value'] # 1,178mW
                        
                        # Use regex to separate the number (note the comma) and the unit
                        match = re.match(r'([\d,]+)([a-zA-Z]+)', laser_power_str)
                        if match:
                            number_str, unit_str = match.groups()
                            # Remove commas and convert to float
                            number = float(number_str.replace(',', '.'))
                            #print(number, unit_str)
                            self.Laser_Power = ureg.Quantity(number, unit_str)
                            
                    if dataxmlfile['Info']['Groups']['Group'][1]['Items']['Item'][3]['Name'] == 'Used Objective':
                        objective_str = dataxmlfile['Info']['Groups']['Group'][1]['Items']['Item'][3]['Value'] # 20x
                    
                        match = re.match(r'([\d,]+)([a-zA-Z]+)', objective_str)
                        if match:
                            number_str, unit_str = match.groups()
                            # Remove commas and convert to float
                            number = float(number_str.replace(',', ''))
                            #print(number, unit_str)
                            self.Ramification_Objective = ureg.Quantity(number, 'dimensionless')
                    
                    if dataxmlfile['Info']['Groups']['Group'][3]['Groups']['Group']['Items']['Item'][6]['Name'] == 'Groove_Density':
                        groove_density_str = dataxmlfile['Info']['Groups']['Group'][3]['Groups']['Group']['Items']['Item'][6]['Value'] # 300 g/mm
                    
                        match = re.match(r'([\d\s]+)([a-zA-Z/]+)', groove_density_str)
                        if match:
                            number_str, unit_str = match.groups()
                            # Remove commas and convert to float
                            number = float(number_str.replace(',', ''))
                            #print(number, unit_str)
                            self.Groove_Density = ureg.Quantity(number, '1/millimeter')
                            
                    if dataxmlfile['Info']['Groups']['Group'][4]['Items']['Item'][4]['Name'] == 'Exposure_Time_(ms)':
                        accumulation_time_str = dataxmlfile['Info']['Groups']['Group'][4]['Items']['Item'][4]['Value'] # 10000
                    
                        match = re.match(r'([\d]+)', accumulation_time_str)
                        if match:
                            number_str = match.group(1)
                            number = float(number_str)
                            self.Accumulation_Time = ureg.Quantity(number, 'millisecond')
                            
                    if dataxmlfile['Info']['Groups']['Group'][4]['Items']['Item'][6]['Name'] == 'No_of_Accumulations':
                        accumulation_number_str = dataxmlfile['Info']['Groups']['Group'][4]['Items']['Item'][6]['Value'] # 6
                        
                        match = re.match(r'([\d]+)', accumulation_number_str)
                        if match:
                            number_str = match.group(1)
                            number = int(number_str)
                            self.No_of_Accumulations = ureg.Quantity(number, 'dimensionless')

                    
                    from datetime import datetime
                    dateExpermimentParsed = datetime.strptime(string_experiment_time,'%d.%m.%Y %H:%M')
                    self.datetime = dateExpermimentParsed
                    
                    ###
                    # List of Intensity counts for every frame in file
                    ###
                    offsetHeader = 0x1538+1*CLXML + 3*4 + 8 + 101
                    
                    # Create subsection if not existing
                    if not self.Raman_data_entries:
                        self.Raman_data_entries = []
                        # Ensure the list is long enough
                        while len(self.Raman_data_entries) < numFrames:
                            self.Raman_data_entries.append(RamanData())  # Append a placeholder value
                    
                    # Create new if not sufficient long enough - overwrites the default
                    if len(self.Raman_data_entries) < numFrames:
                        self.Raman_data_entries = []
                        while len(self.Raman_data_entries) < numFrames:
                            self.Raman_data_entries.append(RamanData())  # Append a placeholder value
                    
                    #print(len(self.Raman_data_entries))
                    
                    # Do this for every frame in file
                    for frame in range(0,numFrames,1):
                        #print(frame)
                        datasplice = contentTVBfile[offsetHeader:offsetHeader+4*NRWE] 
                        # 'f': float (4 byte)
                        count = len(datasplice)//4 # Number of bytes to unpack (1 for char)
                        #print(count)
                        unpacked_data = self.unpack_repeated_bytes(datasplice, 'f', count)
                        #print(unpacked_data)
                        
                        import numpy as np
                        IntensityCount = np.asarray(unpacked_data, dtype=np.float64)
                        #print(IntensityCount)
                        
                        # Separate the columns into two variables and copy to 
                        self.Raman_data_entries[frame].Raman_shift = ureg.Quantity(RamanWavenumber, '1/nanometer')
                        self.Raman_data_entries[frame].Intensity = ureg.Quantity(IntensityCount, 'dimensionless')
                        
                        offsetHeader += 4*NRWE + 3*4 + 8 + 101 # specific after every frame 
                    
                    
            #Check if any file is provided in any subsection for .tvb or .txt files
            for r_d_entries in self.Raman_data_entries:
                if r_d_entries.data_as_tvf_or_txt_file:
                    # Check if the file has the correct extension: TriVista tvf or plain 2-column txt
                    if not r_d_entries.data_as_tvf_or_txt_file.endswith('.tvf') and not r_d_entries.data_as_tvf_or_txt_file.endswith('.txt'):
                        #print("Expect Data File Error")
                        raise DataFileError(f"The file '{r_d_entries.data_as_tvf_or_txt_file}' must have a .tvf or .txt extension.")
                    
                    # Otherwise parse the file with *.txt
                    if r_d_entries.data_as_tvf_or_txt_file.endswith('.txt'):
                        with archive.m_context.raw_file(r_d_entries.data_as_tvf_or_txt_file) as xyfile:
                            # Load the data from the file
                            import numpy as np
                            dataxyfile = np.loadtxt(xyfile)
                            
                            # Separate the columns into two variables and copy to 
                            r_d_entries.Raman_shift = ureg.Quantity(dataxyfile[:, 0], '1/centimeter') # dataxydfile[:, 0]  # First column
                            r_d_entries.Intensity = ureg.Quantity(dataxyfile[:, 1], 'dimensionless') #dataxydfile[:, 1]  # Second column
                    
                    # Otherwise parse the file with *.tvf
                    if r_d_entries.data_as_tvf_or_txt_file.endswith('.tvf'):
                        with archive.m_context.raw_file(r_d_entries.data_as_tvf_or_txt_file) as xyfile:
                            #Load the data from the file
                            contentxyfile = xyfile.read()
                            
                            #use additional packages
                            import xmltodict, json
                            dataxyfile = xmltodict.parse(contentxyfile)
                            
                            unitWave = dataxyfile['XmlMain']['Documents']['Document']['xDim']['Calibration']['@Unit'].lower()
                            calibrationLaserWave = float(dataxyfile['XmlMain']['Documents']['Document']['xDim']['Calibration']['@LaserWave'])
                            r_d_entries.Laser_Excitation_Wavelength = ureg.Quantity(calibrationLaserWave, unitWave)
                            
                            #Read the actual Raman wavelength data
                            RamanWavelength=dataxyfile['XmlMain']['Documents']['Document']['xDim']['Calibration']['@ValueArray']
                            # first number (=int) gives the number of data entries
                            RamanWavelengthData = [int(x) if x.isdigit() else float(x) for x in RamanWavelength.split('|')]
                            
                            ### Conversion from Wavelength[nm] to Wavenumber[1/cm]
                            # $\Delta \omega [cm^{-1}] = ( \frac{1}{\lambda_{laser}} - \frac{1}{\lambda_{}}) \cdot 10â·$
                            
                            import numpy as np
                            RamanWavenumber = (1.0/calibrationLaserWave-1.0/np.asarray(RamanWavelengthData[1:], dtype=np.float64)) # in 1/nm
                            
                            # Read-in of Intensity Counts
                            IntensityString = dataxyfile['XmlMain']['Documents']['Document']['Data']['Frame']['#text']
                            Intensity = np.asarray([float(x) for x in IntensityString.split(';')], dtype=np.float64)
                            
                            # Archive the data
                            r_d_entries.Raman_shift = ureg.Quantity(RamanWavenumber, f'1/{unitWave}') # dataxydfile[:, 0]  # First column
                            r_d_entries.Intensity = ureg.Quantity(Intensity, 'dimensionless') #dataxydfile[:, 1]  # Second column
                            
            # Check if there's any zip file
            if self.processed_data_as_zip_file:
                # Check if the file has the correct extension: zip archive with plain 2-column txt
                if not self.processed_data_as_zip_file.endswith('.zip'):
                    raise DataFileError(f"The file '{self.processed_data_as_zip_file}' must have a .zip extension.")
                
                # Otherwise parse the file
                with archive.m_context.raw_file(self.processed_data_as_zip_file,'rb') as zipf:
                    #print(zipf)
                  #= zipf.open()
                    with zipfile.ZipFile(zipf, 'r') as zipArchiveFile:
                        #print(zipArchiveFile.infolist(), " with length ", len(zipArchiveFile.infolist()))
                        
                        # Get the number of expected datasets
                        number_of_processed_frames = len(zipArchiveFile.infolist())
                        
                        # Create subsection if not existing
                        if not self.Raman_processed_data_entries:
                            self.Raman_processed_data_entries = []
                            # Ensure the list is long enough
                            while len(self.Raman_processed_data_entries) < number_of_processed_frames:
                                self.Raman_processed_data_entries.append(RamanData())  # Append a placeholder value
                        
                        # Create new if not sufficient long enough - overwrites the default
                        if len(self.Raman_processed_data_entries) < number_of_processed_frames:
                            self.Raman_processed_data_entries = []
                            while len(self.Raman_processed_data_entries) < number_of_processed_frames:
                                self.Raman_processed_data_entries.append(RamanData())  # Append a placeholder value
                        
                        for index, file_info in enumerate(zipArchiveFile.infolist()):
                            #print(zipfile.infolist())
                            # Loop over every file
                            with zipArchiveFile.open(file_info) as zipFileContent:
                                #content = zipFileContent.read()#.decode('utf-8')  # Decode bytes to string
                                import numpy as np
                                content = np.loadtxt(zipFileContent)
                                
                                self.Raman_processed_data_entries[index].Raman_shift = ureg.Quantity(content[:, 0], '1/centimeter')
                                
                                self.Raman_processed_data_entries[index].Intensity = ureg.Quantity(content[:, 1], 'dimensionless')
                                
                                if self.Raman_processed_data_entries[index].name is None:
                                    self.Raman_processed_data_entries[index].name = file_info.filename
                                #print(f'Content of {file_info.filename}:\n{content}\n')

            
        except Exception as e:
            logger.error('Invalid file parsing error.', exc_info=e)
        
        # if self.Raman_data_entries:
        #Otherwise create plot
        self.figures = self.generate_plots()
        
        super().normalize(archive, logger)



class MeasurementAdsorption(ELNMeasurement, PlotSection, ArchiveSection):
    '''
    Class for handling measurement of Adsorption.
    '''
    m_def = Section(
        categories=[CRC1415Category],
        label='CRC1415-Measurement-Adsorption',
        a_eln={
            "overview": True,
            "hide": [
                "name",
                "lab_id",
                "method",
                "samples",
                "measurement_identifiers"
            ],
            "properties": {
                "order": [
                    "tags",
                    "datetime",
                    "datetime_end",
                    "location",
                    "data_as_txt_file",
                    "description"
                ]
            }
        },
    )
    lab_id = Quantity(
        type=str,
        a_display={
            "visible": False
        },
    )
    data_as_txt_file = Quantity(
        type=str,
        description="A reference to an uploaded Quantachrome .txt produced by the adsorption instrument.",
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity"
        },
    )
    
    datetime_end = Quantity(
        type=Datetime,
        description='The date and time when this activity has ended.',
        a_eln=dict(component='DateTimeEditQuantity', label='Ending Time'),
    )
    
    Analysis_Time= Quantity(
        type=np.float64,
        unit='minute',
        description='The time for performing the analysis.',
        a_eln=dict(component='NumberEditQuantity', label='Analysis Time', defaultDisplayUnit= 'minute'),
    )
    
    Sample_Weight = Quantity(
        type=np.float64,
        unit='milligram',
        description='The weight of the sample, milligram',
        a_eln=dict(component='NumberEditQuantity', label='Sample Weight', defaultDisplayUnit= 'milligram'),
    )
    
    Outgas_Time= Quantity(
        type=np.float64,
        unit='hour',
        description='The time during the outgas process.',
        a_eln=dict(component='NumberEditQuantity', label='Outgas Time', defaultDisplayUnit= 'hour'),
    )
    
    Outgas_Temperature = Quantity(
        type=np.float64,
        unit='celsius',
        description='The temperature during the outgas process.',
        a_eln=dict(component='NumberEditQuantity', label='Outgas Temperature', defaultDisplayUnit= 'celsius'),
    )
    
    Analysis_Gas  = Quantity(
        type=str,
        #unit='celsius',
        description='The gas used for the analysis.',
        a_eln=dict(component='StringEditQuantity', label='Analysis Gas'),
    )
    
    Bath_Temperature = Quantity(
        type=np.float64,
        unit='kelvin',
        description='The temperature of the bath.',
        a_eln=dict(component='NumberEditQuantity', label='Bath Temperature', defaultDisplayUnit= 'kelvin'),
    )
    
    RelativePressure = Quantity(
        type=np.float64,
        shape=["*"],
        unit='dimensionless',
        description='The relative pressure range of the spectrogram, dimensionless.',
    )
    AdsorpedVolume = Quantity(
        type=np.float64,
        shape=["*"],
        unit='millimole/gram',
        description='The measured adsorped volume at relative pressure value, normalized by ideal gas molar volume 22.4 cm**3/mol.',
    )
    
    def generate_plots(self) -> list[PlotlyFigure]:
        """
        Generate the plotly figures for the `MeasurementAdsorption` section.

        Returns:
            list[PlotlyFigure]: The plotly figures.
        """
        # figures = []
        # #if self.wavelength is None:
        # #    return figures
        # 
        # x_label = 'Wavenumber'
        # xaxis_title = f'{x_label} (cm-1)'
        # x = self.Wavenumber.to('1/cm').magnitude
        # 
        # y_label = 'Transmittance'
        # yaxis_title = f'{y_label} (a.u.)'
        # y = self.Transmittance.to('dimensionless').magnitude
        # 
        # line_linear = px.line(x=x, y=y)
        # 
        # line_linear.update_layout(
        #     title=f'{y_label} over {x_label}',
        #     xaxis_title=xaxis_title,
        #     yaxis_title=yaxis_title,
        #     xaxis=dict(
        #         fixedrange=False,
        #     ),
        #     yaxis=dict(
        #         fixedrange=False,
        #     ),
        #     template='plotly_white',
        # )
        # 
        # figures.append(
        #     PlotlyFigure(
        #         label=f'{y_label} linear plot',
        #         index=0,
        #         figure=line_linear.to_plotly_json(),
        #     ),
        # )

        return figures
    
    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger'):
        """
        The normalize function of the `MeasurementIR` section.

        Args:
            archive (EntryArchive): The archive containing the section that is being
            normalized.
            logger (BoundLogger): A structlog logger.
        """
        
        try:
            # Check if any file is provided
            if self.data_as_txt_file:
                # Check if the file has the correct extension
                if not self.data_as_txt_file.endswith('.txt'):
                    raise DataFileError(f"The file '{self.data_as_txt_file}' must have a Quantachrome .txt extension.")
            
                # Get the encoding
                # from chardet import detect # for Quantachrome ASiQwin text as the file has the 'Windows-1252' encoding
                # # but this can usally ignored as the only character is the Angstrom 
                
                # with archive.m_context.raw_file(self.data_as_txt_file, 'rb') as txtfile:
                #     rawdata = txtfile.read()
                #     get_encoding_type = detect(rawdata)['encoding']
                #     print(get_encoding_type)
                
                with archive.m_context.raw_file(self.data_as_txt_file, 'r', errors='ignore') as txtfile:
                    text = txtfile.read()

                # Merge all whitespaces to one
                cleaned_text = re.sub(r'\s+', ' ', text)

                # Split the text to find the section starting with "Analysis Report"
                report_start = cleaned_text.find("Analysis Report")
                if report_start == -1:
                    raise DataFileError(f"The file '{self.data_as_txt_file}' could not be parsed. Error in parsing Analysis Report section.")
                
                # Check for version
                header_text = cleaned_text[:report_start]

                match = re.search(r'version\s+([\d.]+)', header_text, re.IGNORECASE)

                # Check if the match was successful and extract the version
                if match:
                    if match.group(1) != '3.01' and match.group(1) != '3.0':
                        raise DataFileError(f"The file '{self.data_as_txt_file}' could not be parsed. Parser only for v3.0 instead {match.group(1) } found.")
                else:
                    raise DataFileError(f"The file '{self.data_as_txt_file}' could not be parsed. No version information found.")
                
                
                # Extract the relevant part of the text
                report_text = cleaned_text[report_start:]
                
                # Define a dictionary to hold the extracted fields
                report_data = {}
                
                # Use regular expressions to extract the fields
                #report_data['Operator'] = re.search(r'Operator:\s*(.*?)\s*Date:', report_text).group(1).strip()
                #report_data['Date'] = re.search(r'Date:\s*(.*?)\s*Sample ID:', report_text).group(1).strip()
                #report_data['Sample ID'] = re.search(r'Sample ID:\s*(.*?)\s*Filename:', report_text).group(1).strip()
                #report_data['Filename'] = re.search(r'Filename:\s*(.*?)\s*Sample Desc:', report_text).group(1).strip()
                #report_data['Sample Desc'] = re.search(r'Sample Desc:\s*(.*?)\s*Sample weight:', report_text).group(1).strip()
                #report_data['Sample weight'] = re.search(r'Sample weight:\s*(.*?)\s*Analysis Time:', report_text).group(1).strip()
                report_data['Sample weight'] = re.search(r'Sample weight:\s*([\d.]+)\s*(\w+)', report_text, re.IGNORECASE).group(1).strip()
                report_data['Sample weight unit'] = re.search(r'Sample weight:\s*([\d.]+)\s*(\w+)', report_text, re.IGNORECASE).group(2).strip()
    
                
                #report_data['Analysis Time'] = re.search(r'Analysis Time:\s*(.*?)\s*End of run:', report_text).group(1).strip()
                
                # Regex pattern to match both formats either
                # MMM.M min or HH:MM hr:min
                pattern_analysis_time = r'Analysis Time:\s*([\d.]+)\s*min|Analysis Time:\s*(\d+):(\d+)\s*hr:min'

                match_analysis_time = re.search(pattern_analysis_time, report_text, re.IGNORECASE)

                if match_analysis_time:
                    if match_analysis_time.group(1):  # format matched MMM.M min
                        report_data['Analysis Time'] = float(match_analysis_time.group(1))
                        report_data['Analysis Time unit'] = 'minutes'
                    elif match_analysis_time.group(2) and match_analysis_time.group(3):  # format matched HH:MM hr:min
                        report_data['Analysis Time'] = float(match_analysis_time.group(2))*60.0 + float(match_analysis_time.group(3)) # convert in minutes
                        report_data['Analysis Time unit'] = 'minutes'
                        #print(f"{match_analysis_time.group(2)} hr {match_analysis_time.group(3)} min")
                
                #report_data['End of run'] = re.search(r'End of run:\s*(.*?)\s*Instrument:', report_text).group(1).strip()
                
                # Regex pattern to match the date and time MM/DD/YYYY HH:MM:SS
                pattern_end_run = r'End of run:\s*(\d{1,2}/\d{1,2}/\d{4}\s+\d{1,2}:\d{2}:\d{2})'
                
                # Use regex to find the date and time
                match_end_run = re.search(pattern_end_run, report_text)
                
                # Check if the match was successful and extract the date and time
                if match_end_run:
                    #print("Parsed:", match_end_run.group(0))
                    report_data['End of run'] = match_end_run.group(1)
                
                #report_data['Instrument'] = re.search(r'Instrument:\s*(.*?)\s*Void Vol.:', report_text).group(1).strip()
                #report_data['Void Vol.'] = re.search(r'Void Vol.:\s*(.*?)\s*He Mode.Cell:', report_text).group(1).strip()
                #report_data['He Mode.Cell'] = re.search(r'He Mode.Cell:\s*(.*?)\s*Run mode', report_text).group(1).strip()
                #report_data['Run mode'] = re.search(r'Run mode(.*?)(Instrument version:)', report_text).group(1).strip()
                #report_data['Instrument version'] = re.search(r'Instrument version:\s*(.*?)\s*Thermal delay:', report_text).group(1).strip()
                #report_data['Thermal delay'] = re.search(r'Thermal delay:\s*(.*?)\s*He evac time:', report_text).group(1).strip()
                #report_data['He evac time'] = re.search(r'He evac time:\s*(.*?)\s*Outgas Time:', report_text).group(1).strip()
                #print('outgas0')
                #report_data['Outgas Time'] = re.search(r'Outgas Time:\s*(.*?)\s*OutgasTemp:', report_text).group(1).strip()
                report_data['Outgas Time'] = re.search(r'Outgas Time:\s*([\d.]+)\s*(\w+)', report_text, re.IGNORECASE).group(1)
                report_data['Outgas Time unit'] = re.search(r'Outgas Time:\s*([\d.]+)\s*(\w+)', report_text, re.IGNORECASE).group(2)
                
                #report_data['OutgasTemp'] = re.search(r'OutgasTemp:\s*(.*?)\s*Analysis gas:', report_text).group(1).strip()
                report_data['OutgasTemp'] = re.search(r'Outgas\s*Temp\.?:\s*([\d.]+)\s*(\w+)', report_text, re.IGNORECASE).group(1)
                report_data['OutgasTemp unit'] = re.search(r'Outgas\s*Temp\.?:\s*([\d.]+)\s*(\w+)', report_text, re.IGNORECASE).group(2)
                
                #report_data['Analysis gas'] = re.search(r'Analysis gas:\s*(.*?)\s*Bath Temp:', report_text).group(1).strip()
                report_data['Analysis gas'] = re.search(r'Analysis gas:\s*(.*?)\s*(\w+)', report_text, re.IGNORECASE).group(0).split()[2]
                
                #report_data['Bath Temp'] = re.search(r'Bath Temp:\s*(.*?)\s*Press. Tolerance:', report_text).group(1).strip()
                
                report_data['Bath Temp'] = re.search(r'Bath\s*Temp\.?:\s*([\d.]+)\s*(\w+)', report_text, re.IGNORECASE).group(1)
                report_data['Bath Temp unit'] = re.search(r'Bath\s*Temp\.?:\s*([\d.]+)\s*(\w+)', report_text, re.IGNORECASE).group(2)
                #report_data['Press. Tolerance'] = re.search(r'Press. Tolerance:\s*(.*?)\s*Equil time:', report_text).group(1).strip()
                #report_data['Equil time'] = re.search(r'Equil time:\s*(.*?)\s*Equil timeout:', report_text).group(1).strip()
                #report_data['Equil timeout'] = re.search(r'Equil timeout:\s*(.*?)\s*Data Reduction Parameters', report_text).group(1).strip()
                
                # Extract the metadata
                self.Sample_Weight = ureg.Quantity(float(report_data['Sample weight']), report_data['Sample weight unit'])
                
                # Convert the unpacked data as a datetime object
                from dateutil import parser as dataparser
                from datetime import datetime, timedelta
                
                self.Analysis_Time = ureg.Quantity(float(report_data['Analysis Time']), report_data['Analysis Time unit'])
                #print(type(analysistime.to(ureg.minute).magnitude))
                #print(float(analysistime.to(ureg.minute).magnitude))
                
                if 'End of run' in report_data.keys():
                    import pytz
                    # End of run:    MM/DD/YYYY 5:11:04 in Berlin/Europe time zone
                    exp_time = dataparser.parse(report_data['End of run'], dayfirst=False)
                    
                    local_tz = pytz.timezone('Europe/Berlin')
                    target_tz = pytz.timezone('UTC')
                    
                    exp_time = local_tz.localize(exp_time) # set to Berlin time
                    exp_time = target_tz.normalize(exp_time) #transfer to UTC
                    
                    self.datetime_end = exp_time
                    
                    self.datetime = exp_time - timedelta(minutes=float(self.Analysis_Time.to(ureg.minute).magnitude))
                
                self.Outgas_Time = ureg.Quantity(float(report_data['Outgas Time']), 'hour' if report_data['Outgas Time unit'] == 'hrs' else 'dimensionless')
                
                self.Outgas_Temperature = ureg.Quantity(float(report_data['OutgasTemp']), 'celsius' if report_data['OutgasTemp unit'] == 'C' else 'dimensionless')
                
                self.Analysis_Gas = report_data['Analysis gas']
                
                self.Bath_Temperature = ureg.Quantity(float(report_data['Bath Temp']), 'kelvin' if report_data['Bath Temp unit'] == 'K' else 'dimensionless')
                
                
                # Splits the file into parts separated by empty lines.
                # The actual data is in the last part.
                parts = text.split('\n\n')
                
                # The first part is everything before the first empty line
                #before_section = parts[0].strip()
                #print(before_section)
                # The data part is everything after the last empty line
                # We need to check if there is a data part
                data_section = parts[len(parts)-1].strip() if len(parts) > 1 else None 
                
                if not data_section:
                    raise DataFileError(f"The file '{self.data_as_txt_file}' could not be parsed. Error in parsing data section.")
                # Convert table data to numpy array
                relativPressure_array = []
                adsorpedVolume_array = []

                # Split the section into lines
                lines = data_section.splitlines()
                
                # Iterate through each line and extract values
                for line in lines:
                    # Use regex to capture the relative pressure and volume values
                    match = re.match(r'\s*(\S+)\s+(\S+)', line)
                    
                    if match:
                        try:
                            relativPressure_array.append(float(match.group(1)))
                            adsorpedVolume_array.append(float(match.group(2)))
                        except ValueError:
                            pass
                
                relativePressure = np.array(relativPressure_array)
                adsorpedVolume = np.array(adsorpedVolume_array)
                
                # Archive the data
                self.RelativePressure = ureg.Quantity(relativePressure, 'dimensionless')
                self.AdsorpedVolume = ureg.Quantity(adsorpedVolume/22.4, 'millimole/g') # normalized by ideal gas molar volume
                
                # Find the index of the maximum value
                max_index_relativePressure = np.argmax(relativePressure)

                # Split the array into two parts for plotting
                adsorption_relativePressure = relativePressure[:max_index_relativePressure + 1]  # Include the maximum value
                desorption_relativePressure = relativePressure[max_index_relativePressure:]   # Exclude the maximum value
                
                adsorption_adsorpedVolume = adsorpedVolume[:max_index_relativePressure + 1]/22.4  # Include the maximum value
                desorption_adsorpedVolume = adsorpedVolume[max_index_relativePressure:]/22.4   # Exclude the maximum value
                
                # create plot
                figures = []
                
                # Create a figure
                config = {'displayModeBar': True}
                fig = go.Figure()
                
                x_label = 'Relative Pressure'
                xaxis_title = 'p/p0 [dimensionless]'
                x_ads = adsorption_relativePressure
                x_des = desorption_relativePressure
                
                #y_label = '\u25CF Adsorbed Volume and \u25A1 Desorbed Volume'
                y_label = 'Adsorbed Volume'
                yaxis_title = f'Adsorbed Volume [mmol/g] ({self.Analysis_Gas}, {self.Bath_Temperature.to('kelvin').magnitude} {self.Bath_Temperature.units:~})'
                y_ads = adsorption_adsorpedVolume
                y_des = desorption_adsorpedVolume
                
                #line_ads = px.line(x=x_ads, y=y_ads, markers=True, marker_symbol='circle')
                #line_des = px.line(x=x_des, y=y_des, markers=True, marker_symbol='square-open')
                
                # Add the first line with markers
                fig.add_trace(go.Scatter(
                    x=x_ads,
                    y=y_ads,
                    mode='lines+markers',  # 'lines+markers' to show both lines and markers
                    name='adsorption',         # Name of the first line
                    line=dict(color='blue'),  # Line color
                    hovertemplate='(x: %{x}, y: %{y})<extra></extra>',  # Custom hovertemplate
                    marker=dict(size=10, symbol='circle')      # Marker size
                ))

                # Add the second line with markers
                fig.add_trace(go.Scatter(
                    x=x_des,
                    y=y_des,
                    mode='lines+markers',  # 'lines+markers' to show both lines and markers
                    name='desorption',         # Name of the second line
                    line=dict(color='red'),   # Line color
                    hovertemplate='(x: %{x}, y: %{y})<extra></extra>',  # Custom hovertemplate
                    marker=dict(size=10, symbol='square-open')      # Marker size
                ))

                
                fig.update_layout(
                    title=f'{y_label} over {x_label}',
                    xaxis_title=xaxis_title,
                    yaxis_title=yaxis_title,
                    xaxis=dict(
                        fixedrange=False,
                    ),
                    yaxis=dict(
                        fixedrange=False,
                    ),
                    template='plotly_white',
                    showlegend=True,
                    hovermode="x unified", # provides a dashed line and finds the closest point
                )
                
                # figures.append(
                #     PlotlyFigure(
                #         label=f'{y_label}-{x_label}',
                #         #index=0,
                #         figure=fig.to_plotly_json(),
                #     ),
                # )
                
                figure_json = fig.to_plotly_json()
                figure_json['config'] = {'staticPlot': False, 'displayModeBar': True, 'scrollZoom': True, 'responsive': True, 'displaylogo': True, 'dragmode': True}
                
                figures.append(PlotlyFigure(label=f'{y_label}-{x_label} linear plot', figure=figure_json))
                
                self.figures = figures
                
                
        
        except Exception as e:
            logger.error('Invalid file extension for parsing.', exc_info=e)
        # In case something is odd here -> just return
        # if not self.results:
        #    return
        
        super().normalize(archive, logger)

class MeasurementTGA(ELNMeasurement, PlotSection, ArchiveSection):
    '''
    Class for handling measurement of ThermoGravimetricAnalysis.
    '''
    m_def = Section(
        categories=[CRC1415Category],
        label='CRC1415-Measurement-TGA',
        a_eln={
            "overview": True,
            "hide": [
                "name",
                "lab_id",
                "method",
                "samples",
                "measurement_identifiers"
            ],
            "properties": {
                "order": [
                    "tags",
                    "datetime",
                    "location",
                    "data_as_txt_file",
                    "TGA_Sample_Mass",
                    "TGA_Temperature_Start",
                    "TGA_Temperature_End",
                    "TGA_Rate",
                    "description"
                ]
            }
        },
    )
    lab_id = Quantity(
        type=str,
        a_display={
            "visible": False
        },
    )
    data_as_txt_file = Quantity(
        type=str,
        description="A reference to an uploaded Quantachrome .txt produced by the TGA instrument.",
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity",
            "label": "data as .txt file of TGA experiment"
        },
    )
    
    TGA_Rate= Quantity(
        type=np.float64,
        unit='kelvin/minute',
        description='The rate for temperature increase during the TGA, kelvin per minute.',
        a_eln=dict(component='NumberEditQuantity', label='TGA Rate', defaultDisplayUnit= 'kelvin/minute'),
    )
    
    TGA_Temperature_Start = Quantity(
        type=np.float64,
        unit='celsius',
        description='The starting temperature for the TGA, celsius.',
        a_eln=dict(component='NumberEditQuantity', label='TGA Temperature Start', defaultDisplayUnit= 'celsius'),
    )
    
    TGA_Temperature_End = Quantity(
        type=np.float64,
        unit='celsius',
        description='The ending temperature for the TGA, celsius.',
        a_eln=dict(component='NumberEditQuantity', label='TGA Temperature End', defaultDisplayUnit= 'celsius'),
    )
    
    
    TGA_Sample_Mass = Quantity(
        type=np.float64,
        unit='milligram',
        description='The mass of the sample in the TGA, milligram.',
        a_eln=dict(component='NumberEditQuantity', label='TGA Sample Mass', defaultDisplayUnit= 'milligram'),
    )
    
    TGA_Temperature = Quantity(
        type=np.float64,
        shape=["*"],
        unit='celsius',
        description='The temperature in the TGA, celsius.',
        a_eln=dict(defaultDisplayUnit = 'celsius'),
    )
    
    
    TGA_Mass_Subtrate = Quantity(
        type=np.float64,
        shape=["*"],
        unit='dimensionless',
        description='The measured mass at temperature in the TGA, given in percent.',
        a_eln=dict(defaultDisplayUnit = 'dimensionless'),
    )
    
    
    
    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger'):
        """
        The normalize function of the `MeasurementTGA` section.

        Args:
            archive (EntryArchive): The archive containing the section that is being
            normalized.
            logger (BoundLogger): A structlog logger.
        """
        
        try:
            # Check if any file is provided
            if self.data_as_txt_file:
                # Check if the file has the correct extension
                if not self.data_as_txt_file.endswith('.txt'):
                    raise DataFileError(f"The file '{self.data_as_txt_file}' must have a TGA .txt extension.")
            
                # Get the encoding
                # from chardet import detect # for Quantachrome ASiQwin text as the file has the 'Windows-1252' encoding
                # # but this can usally ignored as the only character is the Angstrom 
                
                # with archive.m_context.raw_file(self.data_as_txt_file, 'rb') as txtfile:
                #     rawdata = txtfile.read()
                #     get_encoding_type = detect(rawdata)['encoding']
                #     print(get_encoding_type)
                
                #with archive.m_context.raw_file(self.data_as_txt_file, 'r', errors='ignore') as txtfile:
                with archive.m_context.raw_file(self.data_as_txt_file, 'r', encoding='iso8859-15') as txtfile:
                    # parse the metadata
                    text = txtfile.read()
                    #print(text)
                    
                    mass = re.search(r'#SAMPLE MASS /(\w+):(\d+),(\d+)', text, re.IGNORECASE)
                    
                    if mass:
                        if len(mass.groups()) == 3:
                            # Combine them to form the complete number
                            mass_decimal = f"{mass.group(2)}.{mass.group(3)}"  # Convert to standard decimal format
                            self.TGA_Sample_Mass = ureg.Quantity(float(mass_decimal), mass.group(1).lower()) # decimal[comma]decimal, mg
                    
                    temp_range = re.search(r'#RANGE:(\d+)(Â°C)/([\d.]+)\((K/min)\)/(\d+)(Â°C)', text)
                    #print(temp_range)
                    #print(len(temp_range.groups()))
                    #print(temp_range.groups())
                    
                    if temp_range:
                        if len(temp_range.groups()) == 6: # start, degC, rate, K/min, stop, degC
                            
                            self.TGA_Temperature_Start = ureg.Quantity(float(temp_range.group(1)), temp_range.group(2))  # decimal, degC
                            
                            self.TGA_Rate = ureg.Quantity(float(temp_range.group(3)), temp_range.group(4).lower().replace('k/min', 'kelvin/minute'))  # decimal, K/min
                            
                            self.TGA_Temperature_End = ureg.Quantity(float(temp_range.group(5)), temp_range.group(6))  # decimal, degC
                    
                    # Use regex to extract the date and time: #DATE/TIME: DD.MM.YYYY HH:MM
                    time_exp = re.search(r'#DATE/TIME:(\d{1,2}\.\d{1,2}\.\d{4}\s+\d{1,2}:\d{1,2})', text)
                    #print(time_exp)
                    # Check if a match was found
                    if time_exp:
                        # Convert the unpacked data as a datetime object
                        from datetime import datetime
                        import pytz
                        exp_time = datetime.strptime(time_exp.group(1), "%d.%m.%Y %H:%M")
                        
                        local_tz = pytz.timezone('Europe/Berlin')
                        target_tz = pytz.timezone('UTC')
                        
                        exp_time = local_tz.localize(exp_time) # set to Berlin time
                        exp_time = target_tz.normalize(exp_time) #transfer to UTC
                        
                        self.datetime = exp_time
                        
                
                with archive.m_context.raw_file(self.data_as_txt_file, 'r', encoding='iso8859-15') as txtfile:
                    # Read-in the special encoded file
                    data_tga = np.loadtxt(txtfile, encoding='iso8859-15', delimiter=';', usecols=[0,3])
                    # Unpack the two columns into separate arrays
                    tga_temperature, tga_mass_percent = data_tga[:, 0], data_tga[:, 1]
                    
                    # note: percent as unit is available at pint v0.22 -> this here is v.17
                    self.TGA_Temperature = ureg.Quantity(tga_temperature, 'celsius')
                    self.TGA_Mass_Subtrate = ureg.Quantity(tga_mass_percent, 'dimensionless')
                    
                    # create plot
                    figures = []
                    
                    # Create a figure
                    fig = go.Figure()
                    
                    x_label = 'Temperature'
                    xaxis_title = 'Temperature [\u2103]'
                    
                    y_label = 'Sample Mass'
                    yaxis_title = 'Sample Mass [%]' #f'Adsorbed Volume [mmol/g] ({self.Analysis_Gas}, {self.Bath_Temperature.to('kelvin').magnitude} {self.Bath_Temperature.units:~})'
                    
                    # Add the first line with markers
                    fig.add_trace(go.Scatter(
                        x=np.array([temp.to('celsius').magnitude for temp in self.TGA_Temperature]), #self.TGA_Temperature.to('celsius').magnitude,
                        y=np.array([mass.to_base_units().magnitude for mass in self.TGA_Mass_Subtrate]), #self.TGA_Mass_Subtrate,
                        mode='lines+markers',  # 'lines+markers' to show both lines and markers
                        name='TGA',         # Name of the first line
                        line=dict(color='blue'),  # Line color
                        hovertemplate='(x: %{x}, y: %{y})<extra></extra>',  # Custom hovertemplate
                        marker=dict(size=5, symbol='circle')      # Marker size
                    ))
                    
                    
                    fig.update_layout(
                        title=f'{y_label} over {x_label}',
                        xaxis_title=xaxis_title,
                        yaxis_title=yaxis_title,
                        xaxis=dict(
                            fixedrange=False,
                        ),
                        yaxis=dict(
                            fixedrange=False,
                        ),
                        template='plotly_white',
                        showlegend=True,
                        hovermode="x unified", # provides a dashed line and finds the closest point
                    )
                    
                    # figures.append(
                    #     PlotlyFigure(
                    #         label=f'{y_label}-{x_label}',
                    #         #index=0,
                    #         figure=fig.to_plotly_json(),
                    #     ),
                    # )
                    
                    figure_json = fig.to_plotly_json()
                    figure_json['config'] = {'staticPlot': False, 'displayModeBar': True, 'scrollZoom': True, 'responsive': True, 'displaylogo': True, 'dragmode': True}
                    
                    figures.append(PlotlyFigure(label=f'{y_label}-{x_label} linear plot', figure=figure_json))
                    
                    self.figures = figures
        
        
        except Exception as e:
            logger.error('Invalid file extension for parsing.', exc_info=e)
        # In case something is odd here -> just return
        # if not self.results:
        #    return
        
        super().normalize(archive, logger)


class CVData(ArchiveSection):
    """General data section for cyclic voltammetry"""

    m_def = Section(
        label_quantity='name',
        a_eln={
            # "overview": False,
            # "hide": [
            #     "name",
            #     "lab_id",
            #     "method",
            #     "samples",
            #     "measurement_identifiers"
            # ],
            "properties": {
                "order": [
                    "name",
                    "data_as_txt_file",
                    "CV_Scanrate",
                ]
            }
        },
    )
    
    name = Quantity(
        type=str,
        #default='TestName',
        description='Name of the section or CV measurement',
        a_eln={'component': 'StringEditQuantity', 'label': 'CV: Title of the measurement'},
    )
    
    data_as_txt_file = Quantity(
        type=str,
        description="A reference to an uploaded .txt file produced by the cyclic voltammetry instrument.",
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity"
        },
    )
    
    CV_Scanrate = Quantity(
        type=np.float64,
        unit='volt/second',
        description='The scanrate used in the CV experiment, volt/second.',
        a_eln=dict(component='NumberEditQuantity', label='CV: Scanrate', defaultDisplayUnit= 'volt/second'),
    )
    
    CV_Potential = Quantity(
        type=np.float64,
        shape=["*"],
        unit='volt',
        description='The applied potential during the cyclic voltammetry experiment, volt.',
        a_eln=dict(label='CV Potential', defaultDisplayUnit= 'volt'),
    )
    
    CV_Current = Quantity(
        type=np.float64,
        shape=["*"],
        unit='milliampere',
        description='The measured current during the cyclic voltammetry experiment, milliampere',
        a_eln=dict(label='CV Current', defaultDisplayUnit= 'milliampere'),
    )

class MeasurementCV(ELNMeasurement, PlotSection, ArchiveSection):
    '''
    Class for handling measurement of cyclic voltammetry experiment.
    '''
    m_def = Section(
        categories=[CRC1415Category],
        label='CRC1415-Measurement-CV',
        a_eln={
            "overview": True,
            "hide": [
                "name",
                "lab_id",
                "method",
                "samples",
                "measurement_identifiers"
            ],
            "properties": {
                "order": [
                    "tags",
                    "datetime",
                    "datetime_end",
                    "location",
                    "CV_Electrolyte",
                    "CV_Electrolyte_Concentration",
                    "CV_pH_Value",
                    "CV_Reference_Electrode",
                    "CV_Counter_Electrode_Material",
                    "CV_Working_Electrode_Material",
                    "data_as_ids_file",
                    "description"
                ]
            }
        },
        )
            
    lab_id = Quantity(
        type=str,
        a_display={
            "visible": False
        },
    )
    
    datetime_end = Quantity(
        type=Datetime,
        description='The date and time when this activity has ended.',
        a_eln=dict(component='DateTimeEditQuantity', label='Ending Time'),
    )
    
    CV_Electrolyte = Quantity(
        type=str,
        #default='TestName',
        description='The electrolytes in cyclic voltammetry facilitate ion transport, enabling electrochemical reaction measurements',
        a_eln=dict(component='EnumEditQuantity', label='CV: Electrolyte', suggestions=['KHCO3', 'KCl', 'Na2SO4']),
    )
    
    CV_Electrolyte_Concentration = Quantity(
        type=np.float64,
        unit='mol/l',
        description='The concentration of the electrolytes in cyclic voltammetry, mol/liter.',
        a_eln=dict(component='NumberEditQuantity', label='CV: Electrolyte Concentration', defaultDisplayUnit= 'mol/liter'),
    )
    
    CV_pH_Value = Quantity(
        type=np.float64,
        unit='dimensionless',
        description='The pH value in cyclic voltammetry experiment, dimensionless.',
        a_eln=dict(component='NumberEditQuantity', label='CV: pH Value', defaultDisplayUnit= 'dimensionless'),
    )
    
    CV_Reference_Electrode = Quantity(
        type=str,
        #default='TestName',
        description='The used reference electrode in the experiment.',
        a_eln=dict(component='EnumEditQuantity', label='CV: Reference Electrode', suggestions=['Ag|AgCl (3M)', 'Ag|AgCl (saturated)', 'Hg/Hg2Cl2 (saturated)', 'RHE (reversible hydrogen electrode)']),
    )
    
    CV_Counter_Electrode_Material = Quantity(
        type=str,
        #default='TestName',
        description='The material, which the counter electrode is made of.',
        a_eln=dict(component='EnumEditQuantity', label='CV: Counter Electrode Material', suggestions=['Platinum wire', 'Platinum mesh', 'Graphite', 'Gold']),
    )
    
    CV_Working_Electrode_Material = Quantity(
        type=str,
        #default='TestName',
        description='The material, which the working electrode is made of.',
        a_eln=dict(component='EnumEditQuantity', label='CV: Working Electrode Material', suggestions=['Graphite', 'Glassy carbon', 'Gold', 'Silver']),
    )
    
    
    data_as_ids_file = Quantity(
        type=str,
        description="A reference to an uploaded cyclic voltammetry .ids file produced by the CV instrument.",
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity"
        },
    )
    
    CV_data_entries = SubSection(section_def=CVData, repeats=True)
    
    
    def generate_plots(self) -> list[PlotlyFigure]:
        """
        Generate the plotly figures for the `MeasurementCV` section.

        Returns:
            list[PlotlyFigure]: The plotly figures.
        """
        figures = []
        # Create the figure
        fig = go.Figure()
        
        #for r_d_entries in self.Raman_data_entries:
        for idx, r_d_entries in enumerate(self.CV_data_entries):
            #print(f"Index {idx}/{(len(self.Raman_data_entries) - 1)}: {r_d_entries}")
            # Add line plots
            x = r_d_entries.CV_Potential.to(r_d_entries.CV_Potential.units).magnitude
            y = r_d_entries.CV_Current.to(r_d_entries.CV_Current.units).magnitude
            
            
            # Get the Viridis color scale
            viridis_colors = px.colors.sequential.Viridis
            
            color_index_line = int(idx / (len(self.CV_data_entries)-1) * (len(viridis_colors) - 1)) if len(self.CV_data_entries) > 1 else 0
            
            fig.add_trace(go.Scatter(
                x=x,
                y=y,
                mode='lines+markers',  # 'lines+markers' to show both lines and markers
                name=f'cycle: {idx}',
                line=dict(color=viridis_colors[color_index_line]), # int(idx / (len(self.Raman_data_entries)) * (len(viridis_colors) - 1))]),
                hovertemplate='(x: %{x}, y: %{y})<extra></extra>',
                marker=dict(size=5, symbol='circle')      # Marker size
            ))

        # exemply use the first entry for the units
        x_label = 'Potential'
        xaxis_title = f'{x_label} ({self.CV_data_entries[0].CV_Potential.units:~})'#(1/cm)' the ':~' gives the short form
        
        y_label = 'Current'
        yaxis_title = f'{y_label} ({self.CV_data_entries[0].CV_Current.units:~})'
        
        fig.update_layout(
            title=f'{y_label} over {x_label} - Cyclic Voltammetry',
            xaxis_title=xaxis_title,
            yaxis_title=yaxis_title,
            xaxis=dict(
                fixedrange=False,
            ),
            yaxis=dict(
                fixedrange=False,
            ),
            #legend=dict(yanchor='top', y=0.99, xanchor='left', x=0.01),
            template='plotly_white',
            showlegend=True,
            hovermode="closest", #"x unified",
            hoverdistance=10,
        )
        
        fig.update_xaxes(showspikes=True,)  # <-- add this line
        fig.update_yaxes(showspikes=True)  # <-- add this line
        
        # figures.append(
        #     PlotlyFigure(
        #         label=f'{y_label}-{x_label} linear plot',
        #         #index=0,
        #         figure=fig.to_plotly_json(),
        #     ),
        # )
        
        figure_json = fig.to_plotly_json()
        figure_json['config'] = {'staticPlot': False, 'displayModeBar': True, 'scrollZoom': True, 'responsive': True, 'displaylogo': True, 'dragmode': True}
        
        figures.append(
            PlotlyFigure(
                label=f'{y_label}-{x_label} linear plot',
                figure=figure_json
            )
        )
        
        self.figures = figures

        return figures
    
    def read_section(self, contentIDSlines, start_line, num_lines):
        section_lines = []
        #with open(file_path, 'r', errors='ignore') as file:
        #contentIDSlines = file_path.readlines()
        for current_line_number, line in enumerate(contentIDSlines, start=1):
            # Check if we are at the starting line
            if current_line_number >= start_line:
                #print(line)
                # Strip whitespace from the line
                stripped_line = line.strip()
                # Check if the line is empty
                if stripped_line == "":
                    break  # Stop if an empty line is encountered
                section_lines.append(stripped_line)  # Add the line to the list
                # Stop if we have read the specified number of lines
                if len(section_lines) >= num_lines:
                    break

        return section_lines
    
    
    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger'):
        """
        The normalize function of the `MeasurementRaman` section.

        Args:
            archive (EntryArchive): The archive containing the section that is being
            normalized.
            logger (BoundLogger): A structlog logger.
        """
        # super().normalize(archive, logger)
        try:
            #Check if there's any CV .ids file provided in main section
            if self.data_as_ids_file:
                if not self.data_as_ids_file.endswith('.ids'):
                    raise DataFileError(f"The file '{self.data_as_ids_file}' must have a .ids extension.")
                
                
                # Otherwise parse the file - ignore the iso8859-15 encoding
                with archive.m_context.raw_file(self.data_as_ids_file,'r', encoding='iso8859-15') as idsfile:
                    positions_startdate = [] # list of data entries position
                    # Load the data from the file
                    contentIDSlines = idsfile.readlines()  # Read all lines into a listidsfile.read()
                    
                    # Read the file line by line and search for 'primary_data'
                    for line_number, line in enumerate(contentIDSlines):
                        #start_index = 0
                        #while True:
                            # Find the next occurrence of the search string
                        found_data = line.find('starttime=')
                            #if found == -1:
                            #    break  # No more occurrences in this line
                            # Store the position as a tuple (line_number, character_position)
                        if found_data >= 0:
                            # first occurrence: start time and date of frame
                            # next line: end time and date of frame
                            positions_startdate.append((line_number, contentIDSlines[line_number], contentIDSlines[line_number+1]))  
                            #start_index += 1  # Move to the next character to continue searching
                            
                    #print(positions_startdate)
                    
                    # Convert Europe/Berlin to UTC
                    import pytz
                    from dateutil import parser as dataparser 
                    # dates:    DD.MM.YYYY HH:MM:SS in Berlin/Europe time zone
                    starttime = positions_startdate[1][1].strip().split("=")[1] # starttime=14.10.2022 16:42:50 -> 14.10.2022 16:42:50
                    exp_time_start = dataparser.parse(starttime, dayfirst=True)
                    
                    local_tz = pytz.timezone('Europe/Berlin')
                    target_tz = pytz.timezone('UTC')
                    
                    exp_time_start = local_tz.localize(exp_time_start) # set to Berlin time
                    exp_time_start = target_tz.normalize(exp_time_start) #transfer to UTC
                    
                    self.datetime = exp_time_start
                    
                    endtime = positions_startdate[len(positions_startdate)-1][2].strip().split("=")[1]
                    exp_time_end = dataparser.parse(endtime, dayfirst=True)
                    
                    exp_time_end = local_tz.localize(exp_time_end) # set to Berlin time
                    exp_time_end = target_tz.normalize(exp_time_end) #transfer to UTC
                    
                    self.datetime_end = exp_time_end
                
                ###
                # Find the 'Scanrate=' in .ids file
                # Find the 'Title=' in .ids file
                ###
                
                positions_scanrate_data = [] # list of data entries position of keyword 'Scanrate=' in Volt/second
                positions_title_data = [] # list of data entries position of keyword 'Title=' for every cycle
                
                # Otherwise parse the file - ignore the iso8859-15 encoding
                with archive.m_context.raw_file(self.data_as_ids_file,'r', encoding='iso8859-15') as idsfile:
                    # Load the data from the file
                    contentIDSlines = idsfile.readlines()  # Read all lines into a listidsfile.read()
                    
                    # Read the file line by line and search for 'primary_data'
                    for line_number, line in enumerate(contentIDSlines):
                        found_data_scanrate = line.find('Scanrate=')
                        found_data_title = line.find('Title=')
                        
                        # Important: the line starts (== 0) with the keyword
                        if found_data_scanrate == 0:
                            positions_scanrate_data.append((line_number, line.strip().split('=')[1]) )  
                        
                        if found_data_title == 0:
                            positions_title_data.append((line_number, line.strip().split('=')[1]) )  

                
                ###
                # Find the position of the data 
                ###
                
                positions_primary_data = [] # list of data entries position
                
                # Otherwise parse the file - ignore the iso8859-15 encoding
                with archive.m_context.raw_file(self.data_as_ids_file,'r', encoding='iso8859-15') as idsfile:
                    # Load the data from the file
                    contentIDSlines = idsfile.readlines()  # Read all lines into a listidsfile.read()
                    
                    # Read the file line by line and search for 'primary_data'
                    for line_number, line in enumerate(contentIDSlines):
                        #start_index = 0
                        #while True:
                            # Find the next occurrence of the search string
                        found_data = line.find('primary_data')
                            #if found == -1:
                            #    break  # No more occurrences in this line
                            # Store the position as a tuple (line_number, character_position)
                        if found_data >= 0:
                            positions_primary_data.append((line_number, int(contentIDSlines[line_number+1].strip('\x00')), int(contentIDSlines[line_number+2].strip()) ))  
                            #start_index += 1  # Move to the next character to continue searching
                        
                #print(positions_primary_data) # the first list is useless
                    
                with archive.m_context.raw_file(self.data_as_ids_file,'r', encoding='iso8859-15') as idsfile:
                    # Load the data from the file
                    contentIDSlines = idsfile.readlines()  # Read all lines into a listidsfile.read()
                    
                    numFrames = len(positions_primary_data)-1 # the first entry is useless
                    # Create subsection if not existing
                    if not self.CV_data_entries:
                        self.CV_data_entries = []
                        # Ensure the list is long enough
                        while len(self.CV_data_entries) < numFrames:
                            self.CV_data_entries.append(CVData())  # Append a placeholder value
                    
                    # Create new if not sufficient long enough - overwrites the default
                    if len(self.CV_data_entries) < numFrames:
                        self.CV_data_entries = []
                        while len(self.CV_data_entries) < numFrames:
                            self.CV_data_entries.append(CVData())  # Append a placeholder value

                    # Do this for every frame in file
                    for frame in range(1,numFrames+1,1): # omit the first entry and add the last
                        section_lines = self.read_section(contentIDSlines, positions_primary_data[frame][0]+4, positions_primary_data[frame][2])
                        
                        import numpy as np
                        dataxyfile = np.loadtxt(section_lines) # convert the section into data
                        
                        # Separate the columns into two variables and copy to 
                        self.CV_data_entries[frame-1].CV_Potential = ureg.Quantity(dataxyfile[:, 0], 'volt')
                        self.CV_data_entries[frame-1].CV_Current = ureg.Quantity(dataxyfile[:, 1], 'ampere')
                        
                        # Provide the Scanrate (in V/s) used in every run
                        self.CV_data_entries[frame-1].CV_Scanrate = ureg.Quantity(float(positions_scanrate_data[frame][1]), 'volt/second')
                        # Provide the Title of every measurement if not present
                        if self.CV_data_entries[frame-1].name is None:
                            self.CV_data_entries[frame-1].name = positions_title_data[frame][1]
                        
                    
            #Check if any file is provided in any subsection for .txt files
            for r_d_entries in self.CV_data_entries:
                if r_d_entries.data_as_txt_file:
                    # Check if the file has the correct extension: txt or plain 2-column txt
                    # if not r_d_entries.data_as_tvf_or_txt_file.endswith('.tvf') and not r_d_entries.data_as_tvf_or_txt_file.endswith('.txt'):
                    #     raise DataFileError(f"The file '{r_d_entries.data_as_tvf_or_txt_file}' must have a .tvf or .txt extension.")
                    
                    # Otherwise parse the file with *.txt - ignore the iso8859-15 encoding
                    #if r_d_entries.data_as_txt_file.endswith('.txt'):
                    with archive.m_context.raw_file(r_d_entries.data_as_txt_file,'r', errors='ignore') as xyfile:
                            # Load the data from the file
                            import numpy as np
                            dataxyfile = np.loadtxt(xyfile, skiprows=1)
                            
                            # Separate the columns into two variables and copy to 
                            r_d_entries.CV_Potential = ureg.Quantity(dataxyfile[:, 1], 'volt') # dataxydfile[:, 0]  # First column
                            r_d_entries.CV_Current = ureg.Quantity(dataxyfile[:, 2], 'ampere') #dataxydfile[:, 1]  # Second column
                    
                    
            
        except Exception as e:
            logger.error('Invalid file extension for parsing.', exc_info=e)
        
        if self.CV_data_entries:
            #Otherwise create plot
            self.figures = self.generate_plots()
        
        super().normalize(archive, logger)



#import runschema.run
#class CRC1415SampleOverview(ELNSubstance, ReadableIdentifiers, EntryData, runschema.run.Run):
class CRC1415SampleOverview(ELNSubstance, ReadableIdentifiers, EntryData, ArchiveSection):

#import runschema.run
#class CRC1415SampleOverview(ReadableIdentifiers, EntryData, runschema.run.Run):
    '''
    Class autogenerated from yaml schema for ELNSubstance.
    '''
    m_def = Section(#validate=False,
        categories=[CRC1415Category],
        label='CRC1415-SampleOverview',
        a_eln={
            "lane_width": '600px',
            "properties": {
                "order": [
                    "tags",
                    "Project_Tags_Synthesis",
                    "Project_Tags_Characterization",
                    "Project_Tags_Theory",
                    "name",
                    "short_name",
                    "institute",
                    "owner",
                    "datetime",
                    "lab_id",
                    "data_as_cif_file",
                    "molecular_formula",
                    "substance_type",
                    "sample_is_from_collaboration",
                    "Sample_reference_from_collaboration",
                    "description",
                    "chemicals"
                ]
            }
        },
    )
    name = Quantity(
        type=str,
        a_eln={
            "component": "StringEditQuantity"
        },
        default="Default Sample Name",
    )
    tags = Quantity(
        type=MEnum(['internal', 'collaboration', 'project', 'other']),
        a_eln={
            "component": "AutocompleteEditQuantity"
        },
        shape=["*"],
    )
        
    Project_Tags_Synthesis = Quantity(
        type=MEnum(['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A08', 'A10', 'A11']),
        a_eln={
            "component": "AutocompleteEditQuantity",
            "label": "A: Materials Synthesis",
        },
        shape=["*"],
        #a_elasticsearch=Elasticsearch(material_entry_type),
        # see nomad/nomad/datamodel/results.py
    )
    Project_Tags_Characterization = Quantity(
        type=MEnum(['B01', 'B04', 'B06', 'B07', 'B08', 'B09', 'B10', 'B11']),
        a_eln={
            "component": "AutocompleteEditQuantity",
            "label": "B: Characterization",
        },
        shape=["*"],
    )
    Project_Tags_Theory = Quantity(
        type=MEnum(['C01', 'C03', 'C04', 'C06', 'C07', 'C08', 'C09']),
        a_eln={
            "component": "AutocompleteEditQuantity",
            "label": "C: Theory",
        },
        shape=["*"],
    )
    
    data_as_cif_file = Quantity(
        type=str,
        description="A reference to an uploaded .cif file containing the 'Crystallographic Information File'.",
        a_browser={
            "adaptor": "RawFileAdaptor"
        },
        a_eln={
            "component": "FileEditQuantity"
        },
    )
        
    chemicals = Quantity(
        type=CRC1415Chemical,
        a_eln={
            "component": "ReferenceEditQuantity"
        },
        shape=["*"],
    )
    substance_type = Quantity(
        type=MEnum(['crystalline solid', 'powder', 'solution', 'other']),
        a_eln={
            "component": "RadioEnumEditQuantity"
        },
    )
    sample_is_from_collaboration = Quantity(
        type=bool,
        a_eln={
            "component": "BoolEditQuantity"
        },
        default=False,
    )
    Sample_reference_from_collaboration = Quantity(
        type='CRC1415SampleOverview',
        description='If sample is received by collaboration, then reference it here.',
        a_eln={
            "component": "ReferenceEditQuantity"
        },
        shape=["*"],
    )
    
    molecular_formula = Quantity(
        type=str,
        a_eln={
            "component": "StringEditQuantity"
        },
        )
    
    Other_reference_to_measurement = Quantity(
        type=ELNMeasurement,
        description='If sample is measure otherwise, then reference it here.',
        a_eln={
            "component": "ReferenceEditQuantity"
        },
        shape=["*"],
    )
    
    IR_Instrument = SubSection(
        section_def=IRInstrument,
        repeats=True,
    )
    Contributors = SubSection(
        section_def=Contributors,
        repeats=True,
    )
    
    Measurement_CV=SubSection(
       section_def=MeasurementCV,
       repeats=True,
    )
    
    Measurement_Adsorption=SubSection(
       section_def=MeasurementAdsorption,
       repeats=True,
    )
    
    Measurement_IR = SubSection(
        section_def=MeasurementIR,
        repeats=True,
    )
    
    Measurement_Raman =SubSection(
       section_def=MeasurementRaman,
       repeats=True,
    )
    
    Measurement_SEM = SubSection(
        section_def=MeasurementSEM,
        repeats=True,
    )
    
    Measurement_TEM =SubSection(
       section_def=MeasurementTEM,
       repeats=True,
    )
    
    Measurement_TGA =SubSection(
       section_def=MeasurementTGA,
       repeats=True,
    )
    
    Measurement_XRD = SubSection(
        section_def=MeasurementXRD,
        repeats=True,
    )
    
    from crc1415testsample.schema_packages.MeasurementGeneric import MeasurementGeneric
    Measurement_Generic = SubSection(
        section_def=MeasurementGeneric,
        repeats=True,
    )
    

    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -> None:
    #def normalize(self, archive, logger: 'BoundLogger') -> None:
        '''
        The normalizer for the `CRC1415 Sample` class.

        Args:
            archive (EntryArchive): The archive containing the section that is being
            normalized.
            logger (BoundLogger): A structlog logger.
        '''
        super().normalize(archive, logger)
        
        # from runschema.run import Run, Program, TimeRun
        # from runschema.method import Method, ForceField, Model, Interaction, AtomParameters
        # from runschema.system import System, Atoms, AtomsGroup
        # from runschema.calculation import Calculation
        #from nomad.datamodel.metainfo.simulation.system import System, Atoms, AtomsGroup
        #from nomad.datamodel.metainfo.simulation.run import Program, Run
        
        try:
            #Check if there's any .cif file provided in main section
            if self.data_as_cif_file:
                if not self.data_as_cif_file.endswith('.cif'):
                    raise DataFileError(f"The file '{self.data_as_cif_file}' must have a .cif extension.")
                
                from nomad.normalizing import normalizers
                from nomad.normalizing.results import ResultsNormalizer
                #from nomad.normalizing.optimade import OptimadeNormalizer
                

                system_normalizer_cls = None
                for normalizer in normalizers:
                    if normalizer.__name__ == 'SystemNormalizer':
                        system_normalizer_cls = normalizer
                        print(system_normalizer_cls)
                        break
                
                from nomad.datamodel.metainfo import runschema
                #import runschema
                from nomad.normalizing.optimade import OptimadeNormalizer
                from nomad.atomutils import Formula
                from nomad.datamodel.results import Material, System
                from nomad.normalizing.common import nomad_atoms_from_ase_atoms
                from nomad.normalizing.topology import add_system_info, add_system
                
                
                with archive.m_context.raw_file(self.data_as_cif_file,'r') as ciffile:
                    # Load the data from the file
                    #contentIDSlines = ciffile.readlines()  # Read all lines into a listidsfile.read()
                    
                    # see https://github.com/nomad-coe/atomistic-parsers/blob/develop/atomisticparsers/gulp/parser.py#L1058
                    # see https://github.com/nomad-coe/nomad-schema-plugin-run/blob/develop/runschema/system.py
                    # see https://github.com/nomad-coe/atomistic-parsers/blob/develop/atomisticparsers/gromacs/parser.py
                    # https://gitlab.mpcdf.mpg.de/nomad-lab/nomad-FAIR/-/blob/v1.3.16/nomad/datamodel/metainfo/eln/__init__.py?ref_type=tags#L1036
                    from ase.io import read
                    from ase.data import atomic_numbers
                    
                    try:
                        ase_atoms = read(ciffile.name)
                    except Exception as e:
                        raise ValueError('could not read structure file') from e

                    if not archive.results.material:
                        archive.results.material = Material()
                    # formula = Formula(self.molecular_formula)
                    # formula.populate(archive.results.material)

                    # Create a System: this is a NOMAD specific data structure for storing structural
                    # and chemical information that is suitable for both experiments and simulations.
                    #import runschema
                    #runschema.run_schema_entry_point.load()
                    #import runschema.system
                    if runschema:
                        print(runschema)
                        system = System(
                            atoms=nomad_atoms_from_ase_atoms(ase_atoms),
                            label='Structure from file',
                            description='Structure read from the file.',
                            structural_type='bulk',
                            dimensionality='3D',
                        )

                        # archive.results.topology can used to represent relations between systems.
                        # E.g. "System A is part of System B". In our case there is only a single system.
                        topology = {}
                        add_system_info(system, topology)
                        add_system(system, topology)
                        archive.results.material.topology = list(topology.values())
                    
#                     # Read the CIF file
#                     atoms = read(ciffile)
# 
#                     # Extract atomic positions
#                     positionsASE = atoms.get_positions()
# 
#                     # Extract atomic labels (chemical symbols)
#                     labelsASE = atoms.get_chemical_symbols()
#                     
#                     #labels: list[str] = []
#                     #positions: list[list[float]] = []
#                     # Print the atomic labels and positions
#                     #for index, (label_, position_) in enumerate(zip(labelsASE, positionsASE)):
#                     #    labels.append(label_)
#                     #    positions.append([ float(f"{position_[0]}"), float(f"{position_[1]}"), float(f"{position_[2]}")])#     atom.attrib[f"{x}3"]) for x in ["x", "y", "z"]])
#                     #    print(f"Atom {index + 1}: Label = {label}, Position (x, y, z) = {position}")
#                         
#                     if archive.run:
#                         sec_run = archive.run[-1]
#                     else:
#                         sec_run = Run()
#                         archive.run.append(sec_run)
#                     
#                     sec_run.program = Program(name='CIF Import via ASE', version='0.1')
#                     
#                     if sec_run.system:
#                         sec_system = sec_run.system[-1]
#                     else:
#                         sec_system = System()
#                         sec_run.system.append(sec_system)
#                     
#                     if sec_run.method:
#                         sec_method = sec_run.method[-1]
#                     else:
#                         sec_method = Method()
#                         sec_run.method.append(sec_method)
#                     
#                     if sec_run.calculation:
#                         sec_calculation = sec_run.calculation[-1]
#                     else:
#                         sec_calculation = Calculation()
#                         sec_run.calculation.append(sec_calculation)
#                         # archive.run[0].system[0]
#                     #sec_system = System()
#                     #sec_run.system.append(sec_system)
#                     
#                     # sec_run.system.append(
#                     #     System(
#                     #         atoms=Atoms(labels=labels, positions=positions * ureg.angstrom, periodic=[False] * 3),
#                     #         #atoms_group=[self.all_atoms_group],
#                     #         is_representative=True,
#                     #     )
#                     # )
#                    
#                     sec_system.atoms = Atoms(
#                         positions=atoms.get_positions() * ureg.angstrom,
#                         labels=atoms.get_chemical_symbols(),
#                         lattice_vectors=atoms.get_cell() * ureg.angstrom,
#                         periodic=atoms.pbc,
#                         species=[atomic_numbers[symbol] for symbol in atoms.get_chemical_symbols()]
#                     )
#                     
#                     #sec_system.chemical_composition_reduced = atoms.get_chemical_formula()
#                     
#                     #if sec_system.atoms_group is None:
#                     #    sec_system.atoms_group = []
#                     
#                     # Get the unique elements by converting the list to a set
#                     unique_atoms = set(labelsASE)
#                     # Print the unique atoms
#                     #print("Unique Atoms:", unique_atoms)
#                     
#                     # Loop over each unique atom and get their positions
#                     for idx, atom in enumerate(unique_atoms):
#                         # Get the indices of the atoms that match the current unique atom
#                         indices = [i for i, symbol in enumerate(labelsASE) if symbol == atom]
#                         
#                         # Get the positions of these atoms
#                         positions = atoms.get_positions()[indices]
#                         
#                         #print(f"Atom: {atom}")
#                         #print(indices)
#                         
#                         sec_molecule = AtomsGroup()
#                         sec_system.atoms_group.append(sec_molecule)
#                         sec_molecule.index = idx
#                         sec_molecule.atom_indices = indices #np.where(atoms_molnums == molecule)[0]
#                         sec_molecule.n_atoms = len(sec_molecule.atom_indices)
#                         # use first particle to get the moltype
#                         # not sure why but this value is being cast to int, cast back to str
#                         sec_molecule.label = str(labelsASE[sec_molecule.atom_indices[0]])
#                         sec_molecule.type = 'atom'
#                         sec_molecule.is_molecule = False
#                     
#                     #sec_system.is_representative=True
#                     
#                     # sec_run.system.append(
#                     #     System(
#                     #         atoms=Atoms(labels=labels, positions=positions * ureg.angstrom, periodic=[False] * 3),
#                     #         #atoms_group=[self.all_atoms_group],
#                     #         is_representative=True,
#                     #     )
#                     # )
#                     sec_run.normalize(archive, logger)
#                     if system_normalizer_cls:
#                         system_normalizer = system_normalizer_cls(archive)
#                         system_normalizer.normalize()
#                     optimade_normalizer = OptimadeNormalizer()
#                     optimade_normalizer.normalize(archive)
#                     results_normalizer = ResultsNormalizer()
#                     results_normalizer.normalize(archive)
                    
            
        except Exception as e:
            logger.error('Error exception during parsing/processing.', exc_info=e)
        
        # # in case only molecular_formula is provided
        # if self.molecular_formula and self.pure_substance is None:
        #     #if self.substance_name and self.pure_substance is None:
        #     self.pure_substance = PureSubstanceSection(name=self.molecular_formula, molecular_formula=self.molecular_formula)
        #     #self.pure_substance = PubChemPureSubstanceSection(name=self.molecular_formula)
        #     self.pure_substance.normalize(archive, logger)
        # elif self.molecular_formula and self.pure_substance is not None:
        #     # PureSubstanceSection already exists and we need to update
        #     self.pure_substance = None
        #     self.pure_substance = PureSubstanceSection(name=self.molecular_formula, molecular_formula=self.molecular_formula)
        #     self.pure_substance.molecular_formula = self.molecular_formula
        #     # we need to delete manually the results section as
        #     # elements will by populate by System.normalize fct
        #     self.elemental_composition = None
        #     archive.results.material.elements = []
        #     archive.results.material.elemental_composition = []
        #     self.pure_substance.normalize(archive, logger)
            
        #super().normalize(archive, logger)


from nomad.datamodel.metainfo.plot import PlotSection, PlotlyFigure
from nomad.metainfo import Quantity, Section, SubSection, SchemaPackage
from nomad.metainfo.metainfo import SubSection, MEnum
import plotly.express as px
from nomad.datamodel.data import Schema

class MeasurementExample(PlotSection, Schema):
    '''
    Class with PlotSection also shown in OverviewClass.
    '''
    m_def = Section(
        a_eln={
            "overview": True,
        },
    )

    def normalize(self, archive, logger):
        # Otherwise create plot
        fig = px.scatter(x=[0, 1, 2, 3, 4], y=[0, 1, 4, 9, 16])

        self.figures = []
        self.figures.append(PlotlyFigure(label='Measurement Example:', figure=fig.to_plotly_json()))
        super().normalize(archive, logger)


class OverviewClass(Schema):
    '''
    Class for overview of measurement section.
    '''
    m_def = Section()

    Measurement_type = Quantity(
        type=MEnum(['NMR', 'XRD', 'other']),
        a_eln={
            "component": "RadioEnumEditQuantity"
        },
    )

    Measurement_Example = SubSection(
        section_def=MeasurementExample.m_def,
        repeats=True,
    )


m_package.__init_metainfo__()
